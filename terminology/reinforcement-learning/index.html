<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Reinforcement Learning
  #


  什麼是RL
  #

Reinforcement Learning（強化學習）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為。以下是強化學習的幾個應用面向，以及一些相關論文的參考：
遊戲：強化學習在遊戲領域中取得了顯著成就，例如AlphaGo和AlphaZero等。
機器人控制：用於控制機器人完成複雜任務，如行走、抓取物體等。
股票預測：應用於股票交易、投資策略等金融領域。
交通：用於優化交通信號控制、路線規劃等。

  基本概念
  #

強化學習（Reinforcement Learning）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為:

代理（Agent）：在環境中採取行動的實體，可以是機器人、軟件程序等。
環境（Environment）：代理所處的外部世界，提供狀態和反饋給代理。
狀態（State）：環境在某一時刻的描述，通常用$s$ 表示。
行動（Action）：代理在環境中採取的動作，通常用$a$ 表示。
獎勵（Reward）：環境對代理行動的反饋，用於引導學習，通常用$r$ 表示。
策略（Policy）：代理根據狀態選擇行動的方法，通常用$\pi(a|s)$ 表示。
價值函數（Value Function）：評估在某一狀態下遵循特定策略的預期累積獎勵，用於評估狀態的好壞。
Q函數（Q-Function）：評估在某一狀態下採取特定行動後遵循特定策略的預期累積獎勵。


  基本過程
  #


初始化：代理開始與環境交互。
觀察狀態：代理觀察環境的當前狀態。
選擇行動：根據策略選擇行動。
執行動作：在環境中執行動作。
獲得獎勵：環境給予獎勵。
更新知識：更新價值函數或Q函數，以改善未來的決策。


  常見算法
  #


Q-learning：使用Q函數學習最佳行動。
SARSA：使用價值函數學習最佳行動。
Deep Q-Networks (DQN)：使用深度神經網絡來近似Q函數。
Policy Gradient Methods：直接學習策略而非價值函數。


  Value Function VS Q-Function
  #

在強化學習（Reinforcement Learning, RL）中，Value Function（價值函數） 和 Q-Function（Q 值函數） 都是用來評估策略 $\pi$ 的好壞，但它們的側重點不同。

  
      
          函數
          定義
          描述
      
  
  
      
          State Value Function（狀態價值函數）$V(s)$
          $V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, \pi \right]$
          表示 在狀態 $s$ 下，根據策略 $\pi$ 所能期望獲得的累積回報。
      
      
          Action-Value Function（行動價值函數）$Q(s, a)$
          $Q^\pi(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, A_0 = a, \pi \right]$
          表示 在狀態 $s$ 下執行動作 $a$，並按照策略 $\pi$ 行動後，所能期望獲得的累積回報。
      
  



Value Function
$$
V^\pi(s) = \mathbb{E}\pi \left[ \sum{t=0}^\infty \gamma^t r_t \mid s_0 = s \right]
$$"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://bernie6401.github.io/terminology/reinforcement-learning/"><meta property="og:site_name" content="SBK Hugo Site"><meta property="og:title" content="Reinforcement Learning"><meta property="og:description" content="Reinforcement Learning # 什麼是RL # Reinforcement Learning（強化學習）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為。以下是強化學習的幾個應用面向，以及一些相關論文的參考： 遊戲：強化學習在遊戲領域中取得了顯著成就，例如AlphaGo和AlphaZero等。 機器人控制：用於控制機器人完成複雜任務，如行走、抓取物體等。 股票預測：應用於股票交易、投資策略等金融領域。 交通：用於優化交通信號控制、路線規劃等。
基本概念 # 強化學習（Reinforcement Learning）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為:
代理（Agent）：在環境中採取行動的實體，可以是機器人、軟件程序等。 環境（Environment）：代理所處的外部世界，提供狀態和反饋給代理。 狀態（State）：環境在某一時刻的描述，通常用$s$ 表示。 行動（Action）：代理在環境中採取的動作，通常用$a$ 表示。 獎勵（Reward）：環境對代理行動的反饋，用於引導學習，通常用$r$ 表示。 策略（Policy）：代理根據狀態選擇行動的方法，通常用$\pi(a|s)$ 表示。 價值函數（Value Function）：評估在某一狀態下遵循特定策略的預期累積獎勵，用於評估狀態的好壞。 Q函數（Q-Function）：評估在某一狀態下採取特定行動後遵循特定策略的預期累積獎勵。 基本過程 # 初始化：代理開始與環境交互。 觀察狀態：代理觀察環境的當前狀態。 選擇行動：根據策略選擇行動。 執行動作：在環境中執行動作。 獲得獎勵：環境給予獎勵。 更新知識：更新價值函數或Q函數，以改善未來的決策。 常見算法 # Q-learning：使用Q函數學習最佳行動。 SARSA：使用價值函數學習最佳行動。 Deep Q-Networks (DQN)：使用深度神經網絡來近似Q函數。 Policy Gradient Methods：直接學習策略而非價值函數。 Value Function VS Q-Function # 在強化學習（Reinforcement Learning, RL）中，Value Function（價值函數） 和 Q-Function（Q 值函數） 都是用來評估策略 $\pi$ 的好壞，但它們的側重點不同。
函數 定義 描述 State Value Function（狀態價值函數）$V(s)$ $V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, \pi \right]$ 表示 在狀態 $s$ 下，根據策略 $\pi$ 所能期望獲得的累積回報。 Action-Value Function（行動價值函數）$Q(s, a)$ $Q^\pi(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, A_0 = a, \pi \right]$ 表示 在狀態 $s$ 下執行動作 $a$，並按照策略 $\pi$ 行動後，所能期望獲得的累積回報。 Value Function $$ V^\pi(s) = \mathbb{E}\pi \left[ \sum{t=0}^\infty \gamma^t r_t \mid s_0 = s \right] $$"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="terminology"><meta property="article:tag" content="名詞解釋"><title>Reinforcement Learning | SBK Hugo Site</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://bernie6401.github.io/terminology/reinforcement-learning/><link rel=stylesheet href=/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.acdc41c8d39e6c69d70d8a23779875e0a3733fefead3e428d5344966bb12f562.js integrity="sha256-rNxByNOebGnXDYojd5h14KNzP+/q0+Qo1TRJZrsS9WI=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>SBK Hugo Site</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Reinforcement Learning</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#什麼是rl>什麼是RL</a></li><li><a href=#基本概念>基本概念</a><ul><li><a href=#基本過程>基本過程</a></li><li><a href=#常見算法>常見算法</a></li></ul></li><li><a href=#value-function-vs-q-function>Value Function VS Q-Function</a><ul><li><a href=#舉例自駕車停車><strong>舉例：自駕車停車</strong></a></li><li><a href=#總結><strong>總結</strong></a></li></ul></li><li><a href=#on-policy-vs-off-policy>On-Policy VS Off-Policy</a><ul><li><a href=#1-on-policy內部策略學習><strong>1. On-Policy（內部策略學習）</strong></a></li><li><a href=#2-off-policy外部策略學習><strong>2. Off-Policy（外部策略學習）</strong></a></li><li><a href=#3-直觀舉例><strong>3. 直觀舉例</strong></a></li><li><a href=#4-總結on-policy-vs-off-policy><strong>4. 總結：On-Policy vs. Off-Policy</strong></a></li></ul></li><li><a href=#what-is-sarsa--q-learning>What is SARSA & Q-Learning</a><ul><li><a href=#1-sarsaon-policy><strong>1. SARSA（On-Policy）</strong></a></li><li><a href=#2-q-learningoff-policy><strong>2. Q-Learning（Off-Policy）</strong></a></li><li><a href=#3-sarsa-vs-q-learning比較><strong>3. SARSA vs. Q-Learning（比較）</strong></a></li><li><a href=#4-何時用-sarsa何時用-q-learning><strong>4. 何時用 SARSA？何時用 Q-Learning？</strong></a></li><li><a href=#5-總結><strong>5. 總結</strong></a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=reinforcement-learning>Reinforcement Learning
<a class=anchor href=#reinforcement-learning>#</a></h1><h2 id=什麼是rl>什麼是RL
<a class=anchor href=#%e4%bb%80%e9%ba%bc%e6%98%afrl>#</a></h2><p>Reinforcement Learning（強化學習）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為。以下是強化學習的幾個應用面向，以及一些相關論文的參考：
遊戲：強化學習在遊戲領域中取得了顯著成就，例如AlphaGo和AlphaZero等。
機器人控制：用於控制機器人完成複雜任務，如行走、抓取物體等。
股票預測：應用於股票交易、投資策略等金融領域。
交通：用於優化交通信號控制、路線規劃等。</p><h2 id=基本概念>基本概念
<a class=anchor href=#%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5>#</a></h2><p>強化學習（Reinforcement Learning）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為:</p><ol><li><strong>代理（Agent）</strong>：在環境中採取行動的實體，可以是機器人、軟件程序等。</li><li><strong>環境（Environment）</strong>：代理所處的外部世界，提供狀態和反饋給代理。</li><li><strong>狀態（State）</strong>：環境在某一時刻的描述，通常用$s$ 表示。</li><li><strong>行動（Action）</strong>：代理在環境中採取的動作，通常用$a$ 表示。</li><li><strong>獎勵（Reward）</strong>：環境對代理行動的反饋，用於引導學習，通常用$r$ 表示。</li><li><strong>策略（Policy）</strong>：代理根據狀態選擇行動的方法，通常用$\pi(a|s)$ 表示。</li><li><strong>價值函數（Value Function）</strong>：評估在某一狀態下遵循特定策略的預期累積獎勵，用於評估狀態的好壞。</li><li><strong>Q函數（Q-Function）</strong>：評估在某一狀態下採取特定行動後遵循特定策略的預期累積獎勵。</li></ol><h3 id=基本過程>基本過程
<a class=anchor href=#%e5%9f%ba%e6%9c%ac%e9%81%8e%e7%a8%8b>#</a></h3><ol><li><strong>初始化</strong>：代理開始與環境交互。</li><li><strong>觀察狀態</strong>：代理觀察環境的當前狀態。</li><li><strong>選擇行動</strong>：根據策略選擇行動。</li><li><strong>執行動作</strong>：在環境中執行動作。</li><li><strong>獲得獎勵</strong>：環境給予獎勵。</li><li><strong>更新知識</strong>：更新價值函數或Q函數，以改善未來的決策。</li></ol><h3 id=常見算法>常見算法
<a class=anchor href=#%e5%b8%b8%e8%a6%8b%e7%ae%97%e6%b3%95>#</a></h3><ul><li><strong>Q-learning</strong>：使用Q函數學習最佳行動。</li><li><strong>SARSA</strong>：使用價值函數學習最佳行動。</li><li><strong>Deep Q-Networks (DQN)</strong>：使用深度神經網絡來近似Q函數。</li><li><strong>Policy Gradient Methods</strong>：直接學習策略而非價值函數。</li></ul><h2 id=value-function-vs-q-function>Value Function VS Q-Function
<a class=anchor href=#value-function-vs-q-function>#</a></h2><p>在強化學習（Reinforcement Learning, RL）中，<strong>Value Function（價值函數）</strong> 和 <strong>Q-Function（Q 值函數）</strong> 都是用來評估策略 $\pi$ 的好壞，但它們的側重點不同。</p><table><thead><tr><th><strong>函數</strong></th><th><strong>定義</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td><strong>State Value Function（狀態價值函數）$V(s)$</strong></td><td>$V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, \pi \right]$</td><td>表示 <strong>在狀態 $s$ 下，根據策略 $\pi$ 所能期望獲得的累積回報</strong>。</td></tr><tr><td><strong>Action-Value Function（行動價值函數）$Q(s, a)$</strong></td><td>$Q^\pi(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, A_0 = a, \pi \right]$</td><td>表示 <strong>在狀態 $s$ 下執行動作 $a$，並按照策略 $\pi$ 行動後，所能期望獲得的累積回報</strong>。</td></tr></tbody></table><ul><li><p>Value Function
$$
V^\pi(s) = \mathbb{E}<em>\pi \left[ \sum</em>{t=0}^\infty \gamma^t r_t \mid s_0 = s \right]
$$</p><p>其中：</p><ul><li>$\mathbb{E}_\pi$ 表示在策略$\pi$下取期望。</li><li>$r_t$ 是在時間步$t$獲得的獎勵。</li><li>$\gamma$ 是折扣因子，控制未來獎勵的重要性。</li><li>$s_0 = s$ 表示初始狀態為$s$。</li></ul></li><li><p>Q-Function
$$
Q^\pi(s, a) = \mathbb{E}_\pi \left[ r + \gamma V^\pi(s&rsquo;) \mid s, a \right]
$$</p><p>或更一般地：</p><p>$$
Q^\pi(s, a) = \mathbb{E}<em>\pi \left[ \sum</em>{t=0}^\infty \gamma^t r_t \mid s_0 = s, a_0 = a \right]
$$</p><p>其中：</p><ul><li>$s&rsquo;$ 是採取行動$a$後的下一狀態。</li><li>其他符號與價值函數的表達式中相同。</li></ul><p>在Q-learning等算法中，Q函數通常使用以下更新規則來學習：</p><p>$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a&rsquo;} Q(s&rsquo;, a&rsquo;) - Q(s, a) \right]
$$</p><p>其中：</p><ul><li>$\alpha$ 是學習率。</li><li>$\max_{a&rsquo;} Q(s&rsquo;, a&rsquo;)$ 是下一狀態中所有可能行動的最大Q值。</li></ul></li></ul><ul><li><strong>$V(s)$ 只考慮當前狀態的價值，適合評估當前策略的整體表現。</strong></li><li><strong>$Q(s, a)$ 則考慮特定動作的價值，適合用於決策選擇最佳動作（policy improvement）。</strong></li></ul><hr><h3 id=舉例自駕車停車><strong>舉例：自駕車停車</strong>
<a class=anchor href=#%e8%88%89%e4%be%8b%e8%87%aa%e9%a7%95%e8%bb%8a%e5%81%9c%e8%bb%8a>#</a></h3><p>假設我們在訓練一輛 <strong>自動駕駛車</strong> 停進停車位的策略。</p><h4 id=1-狀態價值函數-vs><strong>1. 狀態價值函數 $V(s)$</strong>
<a class=anchor href=#1-%e7%8b%80%e6%85%8b%e5%83%b9%e5%80%bc%e5%87%bd%e6%95%b8-vs>#</a></h4><p>假設 <strong>狀態 $s$</strong> 表示車輛目前的位置，我們有：</p><ul><li>$V(s_1) = 0.8$ 代表在停車場入口時，預計可以獲得 0.8 的回報（可能是停好車的成功率）。</li><li>$V(s_2) = 0.9$ 代表在更接近停車位的位置時，回報變高。</li></ul><p>這表示 <strong>越接近停車位的狀態，價值越高</strong>，但這裡沒有考慮具體的行動選擇。</p><h4 id=2-q-值函數-qs-a><strong>2. Q 值函數 $Q(s, a)$</strong>
<a class=anchor href=#2-q-%e5%80%bc%e5%87%bd%e6%95%b8-qs-a>#</a></h4><p>假設 <strong>動作 $a$</strong> 包括：</p><ul><li><strong>$a_1$ = 前進</strong>（move forward）</li><li><strong>$a_2$ = 左轉</strong>（turn left）</li><li><strong>$a_3$ = 右轉</strong>（turn right）</li></ul><p>我們可以有：</p><ul><li>$Q(s_1, a_1) = 0.6$（在入口時，選擇前進的預期回報是 0.6）</li><li>$Q(s_1, a_2) = 0.2$（在入口時，直接左轉可能導致撞牆，所以回報較低）</li><li>$Q(s_2, a_1) = 0.9$（更接近停車位時選擇前進，回報較高）</li></ul><p>這表示 <strong>Q 值函數不僅考慮當前狀態，還考慮具體的行動對未來回報的影響</strong>。</p><hr><h3 id=總結><strong>總結</strong>
<a class=anchor href=#%e7%b8%bd%e7%b5%90>#</a></h3><ul><li><strong>$V(s)$ 只告訴我們當前狀態好不好，但不告訴我們該做什麼動作。</strong></li><li><strong>$Q(s, a)$ 告訴我們在當前狀態 $s$ 下，選擇不同動作 $a$ 的好壞，可以用來決策選擇最好的動作（如 Q-Learning）。</strong></li></ul><p>🚗 <strong>自駕車例子：</strong></p><ul><li><strong>$V(s)$</strong> 只是說「這個位置靠近停車位，所以好」，但 <strong>不告訴我們該轉向還是前進</strong>。</li><li><strong>$Q(s, a)$</strong> 具體說明「這裡左轉不好，前進較好」，幫助我們選擇最佳動作。</li></ul><p>在 <strong>Q-Learning</strong> 和 <strong>Deep Q-Networks（DQN）</strong> 等方法中，<strong>主要學習 $Q(s, a)$，然後選擇最大 Q 值的動作來更新策略</strong>。</p><h2 id=on-policy-vs-off-policy>On-Policy VS Off-Policy
<a class=anchor href=#on-policy-vs-off-policy>#</a></h2><p>在強化學習（Reinforcement Learning, RL）中，<strong>On-Policy</strong> 和 <strong>Off-Policy</strong> 的區別主要在於<strong>學習時使用的策略（policy）與執行時的策略是否相同</strong>。</p><p>如果用一句話概括：</p><ul><li><strong>On-Policy（內部策略學習）</strong>：學習與執行同一個策略。</li><li><strong>Off-Policy（外部策略學習）</strong>：學習時使用與執行不同的策略。</li></ul><hr><h3 id=1-on-policy內部策略學習><strong>1. On-Policy（內部策略學習）</strong>
<a class=anchor href=#1-on-policy%e5%85%a7%e9%83%a8%e7%ad%96%e7%95%a5%e5%ad%b8%e7%bf%92>#</a></h3><h4 id=定義><strong>定義</strong>
<a class=anchor href=#%e5%ae%9a%e7%be%a9>#</a></h4><ul><li><strong>在收集數據時，使用的策略（行動選擇）與學習時的策略相同。</strong></li><li>也就是說，演算法只能學習當前策略 $\pi$，並依賴 $\pi$ 產生的經驗來改進自身。</li></ul><h4 id=代表性演算法><strong>代表性演算法</strong>
<a class=anchor href=#%e4%bb%a3%e8%a1%a8%e6%80%a7%e6%bc%94%e7%ae%97%e6%b3%95>#</a></h4><ul><li><strong>SARSA</strong>（State-Action-Reward-State-Action）<ul><li>依據當前策略 $\pi$ 來選擇動作並更新 Q 值。</li><li>例如，如果使用 $\epsilon$-貪婪策略（$\epsilon$-greedy），則學習的 Q 值也會考慮這種策略下的行動。</li></ul></li></ul><h4 id=特點><strong>特點</strong>
<a class=anchor href=#%e7%89%b9%e9%bb%9e>#</a></h4><p>✅ <strong>適合策略改進（policy improvement）</strong>，因為它直接學習當前策略的行為。
✅ <strong>收斂性較穩定</strong>，因為學到的價值估計與執行行為相匹配。
❌ <strong>探索能力有限</strong>，因為只能學習自己當前策略的數據，難以學習更好的行動。</p><hr><h3 id=2-off-policy外部策略學習><strong>2. Off-Policy（外部策略學習）</strong>
<a class=anchor href=#2-off-policy%e5%a4%96%e9%83%a8%e7%ad%96%e7%95%a5%e5%ad%b8%e7%bf%92>#</a></h3><h4 id=定義-1><strong>定義</strong>
<a class=anchor href=#%e5%ae%9a%e7%be%a9-1>#</a></h4><ul><li><strong>學習時的策略與執行時的策略不同</strong>，即可以用<strong>不同的策略來收集數據</strong>，然後用這些數據來學習更好的策略。</li><li>這允許模型透過<strong>試探性策略（exploration policy）</strong> 來收集數據，但學習一個更優的<strong>目標策略（target policy）</strong>。</li></ul><h4 id=代表性演算法-1><strong>代表性演算法</strong>
<a class=anchor href=#%e4%bb%a3%e8%a1%a8%e6%80%a7%e6%bc%94%e7%ae%97%e6%b3%95-1>#</a></h4><ul><li><p><strong>Q-Learning</strong></p><ul><li>無論探索時是否選擇了最佳動作，更新 Q 值時都<strong>假設每個狀態都會選擇最優動作（max Q）</strong>，這使得它可以學習最優策略。</li></ul></li><li><p><strong>Deep Q-Networks（DQN）</strong></p><ul><li>使用 <strong>經驗回放（experience replay）</strong>，存儲過去的數據並從中抽樣來訓練 Q 網絡，使得學習與數據收集分離，這本質上是一種 Off-Policy 方法。</li></ul></li></ul><h4 id=特點-1><strong>特點</strong>
<a class=anchor href=#%e7%89%b9%e9%bb%9e-1>#</a></h4><p>✅ <strong>探索能力更強</strong>，因為可以使用不同的策略來收集更多多樣的數據。
✅ <strong>可以利用過去經驗數據</strong>，例如 DQN 的<strong>經驗回放（experience replay）</strong>。
❌ <strong>收斂可能不穩定</strong>，因為學習的策略與數據來源可能不匹配。</p><hr><h3 id=3-直觀舉例><strong>3. 直觀舉例</strong>
<a class=anchor href=#3-%e7%9b%b4%e8%a7%80%e8%88%89%e4%be%8b>#</a></h3><h4 id=例子-1自駕車><strong>例子 1：自駕車</strong>
<a class=anchor href=#%e4%be%8b%e5%ad%90-1%e8%87%aa%e9%a7%95%e8%bb%8a>#</a></h4><p>假設你在訓練<strong>自動駕駛車輛</strong>，你的目標是讓車學習到最安全的駕駛方式：</p><ul><li><p><strong>On-Policy（SARSA）</strong>：</p><ul><li>車輛根據當前駕駛策略行駛，然後學習基於這種駕駛風格的價值函數。例如，如果車輛<strong>偶爾</strong>違規變道，學習到的策略仍然會保留這種行為。</li><li>缺點是如果當前策略不夠優秀，學到的內容可能也不夠優秀。</li></ul></li><li><p><strong>Off-Policy（Q-Learning）</strong>：</p><ul><li>你讓不同車輛<strong>嘗試各種駕駛風格</strong>（包括安全駕駛和激進駕駛），然後學習<strong>最安全的駕駛策略</strong>。</li><li>這樣，即使有些車輛的行為不理想，演算法仍能學習到更好的策略。</li></ul></li></ul><hr><h4 id=例子-2小朋友學騎腳踏車><strong>例子 2：小朋友學騎腳踏車</strong>
<a class=anchor href=#%e4%be%8b%e5%ad%90-2%e5%b0%8f%e6%9c%8b%e5%8f%8b%e5%ad%b8%e9%a8%8e%e8%85%b3%e8%b8%8f%e8%bb%8a>#</a></h4><ul><li><p><strong>On-Policy（SARSA）</strong>：</p><ul><li>小朋友根據自己當前的學習方式練習，例如<strong>只嘗試自己能做的動作</strong>，然後根據這些嘗試來調整策略。</li><li>如果他害怕摔倒，不會嘗試太多冒險的動作，因此學習速度可能較慢。</li></ul></li><li><p><strong>Off-Policy（Q-Learning）</strong>：</p><ul><li>小朋友可以<strong>看別人騎腳踏車</strong>，學習到最好的技巧，然後用來改善自己的策略。</li><li>這樣，即使自己還沒嘗試某些行動，也可以學習到這些行為的價值。</li></ul></li></ul><hr><h3 id=4-總結on-policy-vs-off-policy><strong>4. 總結：On-Policy vs. Off-Policy</strong>
<a class=anchor href=#4-%e7%b8%bd%e7%b5%90on-policy-vs-off-policy>#</a></h3><table><thead><tr><th><strong>比較項目</strong></th><th><strong>On-Policy（內部策略）</strong></th><th><strong>Off-Policy（外部策略）</strong></th></tr></thead><tbody><tr><td><strong>學習策略</strong></td><td>只能學習自身策略的數據</td><td>可以從不同策略的數據中學習</td></tr><tr><td><strong>探索能力</strong></td><td>受限於當前策略的行為</td><td>可以透過不同策略收集更多數據</td></tr><tr><td><strong>代表演算法</strong></td><td>SARSA, REINFORCE</td><td>Q-Learning, DQN</td></tr><tr><td><strong>應用場景</strong></td><td>策略梯度（Policy Gradient）、安全性較高的應用</td><td>需要長期探索、大量數據的應用</td></tr><tr><td><strong>穩定性</strong></td><td>通常比較穩定</td><td>可能會收斂較慢或不穩定</td></tr></tbody></table><p><strong>結論：</strong></p><ul><li><strong>On-Policy（如 SARSA）適合需要安全性與穩定性的學習場景</strong>，但可能學不到最優策略。</li><li><strong>Off-Policy（如 Q-Learning）可以利用更多數據來學習更好的策略</strong>，但可能會不穩定或過度偏向最大 Q 值的選擇。</li></ul><p>如果你想讓模型學到<strong>最好的策略（最優解）</strong>，通常會選擇 <strong>Off-Policy（如 Q-Learning）</strong>。如果你想要學習穩定且不會意外做出危險決策的策略，則會選擇 <strong>On-Policy（如 SARSA）</strong>。</p><h2 id=what-is-sarsa--q-learning>What is SARSA & Q-Learning
<a class=anchor href=#what-is-sarsa--q-learning>#</a></h2><p>:::info
詳細數學推導可以參考: <a href=https://hackmd.io/@RL666/SkNJdfhn9>深度強化學習 Ch3.1 : TD learning</a>
:::
<strong>SARSA（State-Action-Reward-State-Action）</strong> 和 <strong>Q-Learning</strong> 都是強化學習（Reinforcement Learning, RL）中的<strong>值基方法（Value-Based Methods）</strong>，它們的主要目標是<strong>學習動作-價值函數（Q-Function）</strong>，以選擇最佳動作來最大化累積回報。</p><p>它們的核心區別在於<strong>策略選擇方式</strong>：</p><ul><li><strong>SARSA 是 On-Policy（內部策略學習）</strong>，學習當前策略的行為。</li><li><strong>Q-Learning 是 Off-Policy（外部策略學習）</strong>，學習最優策略，即使數據來自不同的行為策略。</li></ul><hr><h3 id=1-sarsaon-policy><strong>1. SARSA（On-Policy）</strong>
<a class=anchor href=#1-sarsaon-policy>#</a></h3><h4 id=核心概念><strong>核心概念</strong>
<a class=anchor href=#%e6%a0%b8%e5%bf%83%e6%a6%82%e5%bf%b5>#</a></h4><p>SARSA 是<strong>On-Policy</strong> 強化學習方法，意味著<strong>它學習的是當前正在執行的策略（policy）</strong>，即它的 Q 值更新是基於實際選擇的動作，而不是理論上的最優動作。</p><h4 id=更新公式><strong>更新公式</strong>
<a class=anchor href=#%e6%9b%b4%e6%96%b0%e5%85%ac%e5%bc%8f>#</a></h4><p>$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
$$
其中：</p><ul><li>$s_t, a_t, r_t$：當前狀態、動作和獎勵。</li><li>$s_{t+1}, a_{t+1}$：下一個狀態及根據當前策略選擇的下一個動作。</li><li>$\alpha$（學習率）：控制更新步伐。</li><li>$\gamma$（折扣因子）：決定未來回報的重要性。</li></ul><h4 id=sarsa-的特點><strong>SARSA 的特點</strong>
<a class=anchor href=#sarsa-%e7%9a%84%e7%89%b9%e9%bb%9e>#</a></h4><p>✅ <strong>學習當前策略的行為</strong>，不會過於激進地選擇最優解，較穩定。
✅ <strong>策略與學習方式一致</strong>，不會突然變成極端的貪婪策略，適合安全性要求高的環境。
❌ <strong>可能學不到最優策略</strong>，如果策略不是最優的，那學習結果也可能不是最優的。</p><h4 id=舉例><strong>舉例</strong>
<a class=anchor href=#%e8%88%89%e4%be%8b>#</a></h4><p><strong>🛵 例子：學騎腳踏車</strong></p><ul><li>小朋友用 SARSA 來學習騎腳踏車，他會根據自己的學習風格來選擇動作（如小心慢騎）。</li><li>當他遇到坑洞時，他會根據自己當前的策略決定是慢慢避開還是稍微加速躲開。</li><li>這代表他學習的是一種「安全但可能不是最快的騎車方式」。</li></ul><hr><h3 id=2-q-learningoff-policy><strong>2. Q-Learning（Off-Policy）</strong>
<a class=anchor href=#2-q-learningoff-policy>#</a></h3><h4 id=核心概念-1><strong>核心概念</strong>
<a class=anchor href=#%e6%a0%b8%e5%bf%83%e6%a6%82%e5%bf%b5-1>#</a></h4><p>Q-Learning 是<strong>Off-Policy</strong> 強化學習方法，這意味著它<strong>學習的是最優策略，而不管數據來自哪種策略</strong>。</p><ul><li>即使當前的行為策略（例如隨機選擇動作）不是最優的，Q-Learning 依然會學習最優的行動選擇方式。</li></ul><h4 id=更新公式-1><strong>更新公式</strong>
<a class=anchor href=#%e6%9b%b4%e6%96%b0%e5%85%ac%e5%bc%8f-1>#</a></h4><p>$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \right]
$$
與 SARSA 唯一的區別是：</p><ul><li>SARSA 用的是「<strong>實際選擇的下一個動作</strong>」$Q(s_{t+1}, a_{t+1})$。</li><li>Q-Learning 直接選擇「<strong>下一個狀態的最優動作</strong>」$\max_{a} Q(s_{t+1}, a)$ 來更新 Q 值。</li></ul><h4 id=q-learning-的特點><strong>Q-Learning 的特點</strong>
<a class=anchor href=#q-learning-%e7%9a%84%e7%89%b9%e9%bb%9e>#</a></h4><p>✅ <strong>能學到最優策略</strong>，因為它總是選擇最好的 Q 值來學習。
✅ <strong>可以利用 Off-Policy 數據</strong>，例如過去的經驗來更新學習結果（像 DQN 的經驗回放）。
❌ <strong>可能學習不穩定</strong>，如果環境變化大，它可能會過度偏向最優動作而忽略探索。</p><h4 id=舉例-1><strong>舉例</strong>
<a class=anchor href=#%e8%88%89%e4%be%8b-1>#</a></h4><p><strong>🚗 例子：自駕車找停車位</strong></p><ul><li>假設 Q-Learning 訓練一輛<strong>自動駕駛車</strong>來找停車位。</li><li>雖然車輛一開始可能亂選路線，但最終它會學到「<strong>最短最快的停車路線</strong>」。</li><li>即使一開始的數據來自於人類駕駛或隨機行為，Q-Learning 仍然會學習出<strong>最優的停車策略</strong>。</li></ul><hr><h3 id=3-sarsa-vs-q-learning比較><strong>3. SARSA vs. Q-Learning（比較）</strong>
<a class=anchor href=#3-sarsa-vs-q-learning%e6%af%94%e8%bc%83>#</a></h3><table><thead><tr><th><strong>比較項目</strong></th><th><strong>SARSA（On-Policy）</strong></th><th><strong>Q-Learning（Off-Policy）</strong></th></tr></thead><tbody><tr><td><strong>學習策略</strong></td><td>學習當前執行的策略</td><td>學習最優策略</td></tr><tr><td><strong>更新方式</strong></td><td>依據實際選擇的行動更新</td><td>依據最大 Q 值更新</td></tr><tr><td><strong>穩定性</strong></td><td>收斂較穩定，但不一定最優</td><td>可能不穩定，但學習結果更好</td></tr><tr><td><strong>探索能力</strong></td><td>受限於當前策略的行為</td><td>可以從不同策略的數據中學習</td></tr><tr><td><strong>代表性應用</strong></td><td>需要安全性高的應用，如醫療機器人</td><td>需要找到最優解的應用，如自駕車</td></tr><tr><td><strong>適用場景</strong></td><td>策略梯度、保守決策環境</td><td>需要大量探索、目標最優化的環境</td></tr></tbody></table><h4 id=核心區別><strong>核心區別</strong>
<a class=anchor href=#%e6%a0%b8%e5%bf%83%e5%8d%80%e5%88%a5>#</a></h4><ol><li><strong>SARSA 是 On-Policy，學習的是「當前策略」，適合安全性較高的情境（如自駕車減速避讓行人）。</strong></li><li><strong>Q-Learning 是 Off-Policy，學習的是「最優策略」，適合追求最優解的環境（如最快找到停車位）。</strong></li></ol><hr><h3 id=4-何時用-sarsa何時用-q-learning><strong>4. 何時用 SARSA？何時用 Q-Learning？</strong>
<a class=anchor href=#4-%e4%bd%95%e6%99%82%e7%94%a8-sarsa%e4%bd%95%e6%99%82%e7%94%a8-q-learning>#</a></h3><p>✅ <strong>SARSA（On-Policy）適用情境</strong></p><ul><li>需要安全性較高的場景，例如：<ul><li><strong>醫療機器人</strong>（避免風險操作）</li><li><strong>自駕車避障</strong>（減少風險行為）</li><li><strong>教育 AI</strong>（確保不過於極端行為）</li></ul></li><li>主要用於「<strong>學習當前策略的行為</strong>」。</li></ul><p>✅ <strong>Q-Learning（Off-Policy）適用情境</strong></p><ul><li>需要找到全局最優解，例如：<ul><li><strong>遊戲 AI</strong>（學到最佳策略）</li><li><strong>自駕車尋找最佳路徑</strong>（找最短停車路線）</li><li><strong>金融交易策略</strong>（學到最賺錢的投資方式）</li></ul></li><li>主要用於「<strong>學習最優策略，而不管數據來源</strong>」。</li></ul><hr><h3 id=5-總結><strong>5. 總結</strong>
<a class=anchor href=#5-%e7%b8%bd%e7%b5%90>#</a></h3><ul><li><strong>SARSA：On-Policy，學當前策略，較安全但可能不是最優解。</strong></li><li><strong>Q-Learning：Off-Policy，學最優策略，探索能力強但可能學習不穩定。</strong></li><li><strong>如果你希望 AI 探索最優解，選 Q-Learning；如果你希望 AI 行為穩定，選 SARSA。</strong></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#什麼是rl>什麼是RL</a></li><li><a href=#基本概念>基本概念</a><ul><li><a href=#基本過程>基本過程</a></li><li><a href=#常見算法>常見算法</a></li></ul></li><li><a href=#value-function-vs-q-function>Value Function VS Q-Function</a><ul><li><a href=#舉例自駕車停車><strong>舉例：自駕車停車</strong></a></li><li><a href=#總結><strong>總結</strong></a></li></ul></li><li><a href=#on-policy-vs-off-policy>On-Policy VS Off-Policy</a><ul><li><a href=#1-on-policy內部策略學習><strong>1. On-Policy（內部策略學習）</strong></a></li><li><a href=#2-off-policy外部策略學習><strong>2. Off-Policy（外部策略學習）</strong></a></li><li><a href=#3-直觀舉例><strong>3. 直觀舉例</strong></a></li><li><a href=#4-總結on-policy-vs-off-policy><strong>4. 總結：On-Policy vs. Off-Policy</strong></a></li></ul></li><li><a href=#what-is-sarsa--q-learning>What is SARSA & Q-Learning</a><ul><li><a href=#1-sarsaon-policy><strong>1. SARSA（On-Policy）</strong></a></li><li><a href=#2-q-learningoff-policy><strong>2. Q-Learning（Off-Policy）</strong></a></li><li><a href=#3-sarsa-vs-q-learning比較><strong>3. SARSA vs. Q-Learning（比較）</strong></a></li><li><a href=#4-何時用-sarsa何時用-q-learning><strong>4. 何時用 SARSA？何時用 Q-Learning？</strong></a></li><li><a href=#5-總結><strong>5. 總結</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>