<!doctype html><html lang=en-us dir=ltr itemscope itemtype=http://schema.org/Article data-r-output-format=html><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.145.0"><meta name=generator content="Relearn 7.6.0+b932d301d7838f3c1a50e318e216b55ce5cc9148"><meta name=description content="Reinforcement Learning 什麼是RL Reinforcement Learning（強化學習）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為。以下是強化學習的幾個應用面向，以及一些相關論文的參考： 遊戲：強化學習在遊戲領域中取得了顯著成就，例如AlphaGo和AlphaZero等。 機器人控制：用於控制機器人完成複雜任務，如行走、抓取物體等。 股票預測：應用於股票交易、投資策略等金融領域。 交通：用於優化交通信號控制、路線規劃等。
基本概念 強化學習（Reinforcement Learning）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為:
代理（Agent）：在環境中採取行動的實體，可以是機器人、軟件程序等。 環境（Environment）：代理所處的外部世界，提供狀態和反饋給代理。 狀態（State）：環境在某一時刻的描述，通常用$s$ 表示。 行動（Action）：代理在環境中採取的動作，通常用$a$ 表示。 獎勵（Reward）：環境對代理行動的反饋，用於引導學習，通常用$r$ 表示。 策略（Policy）：代理根據狀態選擇行動的方法，通常用$\pi(a|s)$ 表示。 價值函數（Value Function）：評估在某一狀態下遵循特定策略的預期累積獎勵，用於評估狀態的好壞。 Q函數（Q-Function）：評估在某一狀態下採取特定行動後遵循特定策略的預期累積獎勵。 基本過程 初始化：代理開始與環境交互。 觀察狀態：代理觀察環境的當前狀態。 選擇行動：根據策略選擇行動。 執行動作：在環境中執行動作。 獲得獎勵：環境給予獎勵。 更新知識：更新價值函數或Q函數，以改善未來的決策。 常見算法 Q-learning：使用Q函數學習最佳行動。 SARSA：使用價值函數學習最佳行動。 Deep Q-Networks (DQN)：使用深度神經網絡來近似Q函數。 Policy Gradient Methods：直接學習策略而非價值函數。 Value Function VS Q-Function 在強化學習（Reinforcement Learning, RL）中，Value Function（價值函數） 和 Q-Function（Q 值函數） 都是用來評估策略 $\pi$ 的好壞，但它們的側重點不同。
函數 定義 描述 State Value Function（狀態價值函數）$V(s)$ $V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, \pi \right]$ 表示 在狀態 $s$ 下，根據策略 $\pi$ 所能期望獲得的累積回報。 Action-Value Function（行動價值函數）$Q(s, a)$ $Q^\pi(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, A_0 = a, \pi \right]$ 表示 在狀態 $s$ 下執行動作 $a$，並按照策略 $\pi$ 行動後，所能期望獲得的累積回報。 Value Function $$ V^\pi(s) = \mathbb{E}\pi \left[ \sum{t=0}^\infty \gamma^t r_t \mid s_0 = s \right] $$"><meta name=author content><meta name=twitter:card content="summary"><meta name=twitter:title content="Reinforcement Learning :: SBK Hugo Site"><meta name=twitter:description content="Reinforcement Learning 什麼是RL Reinforcement Learning（強化學習）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為。以下是強化學習的幾個應用面向，以及一些相關論文的參考： 遊戲：強化學習在遊戲領域中取得了顯著成就，例如AlphaGo和AlphaZero等。 機器人控制：用於控制機器人完成複雜任務，如行走、抓取物體等。 股票預測：應用於股票交易、投資策略等金融領域。 交通：用於優化交通信號控制、路線規劃等。
基本概念 強化學習（Reinforcement Learning）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為:
代理（Agent）：在環境中採取行動的實體，可以是機器人、軟件程序等。 環境（Environment）：代理所處的外部世界，提供狀態和反饋給代理。 狀態（State）：環境在某一時刻的描述，通常用$s$ 表示。 行動（Action）：代理在環境中採取的動作，通常用$a$ 表示。 獎勵（Reward）：環境對代理行動的反饋，用於引導學習，通常用$r$ 表示。 策略（Policy）：代理根據狀態選擇行動的方法，通常用$\pi(a|s)$ 表示。 價值函數（Value Function）：評估在某一狀態下遵循特定策略的預期累積獎勵，用於評估狀態的好壞。 Q函數（Q-Function）：評估在某一狀態下採取特定行動後遵循特定策略的預期累積獎勵。 基本過程 初始化：代理開始與環境交互。 觀察狀態：代理觀察環境的當前狀態。 選擇行動：根據策略選擇行動。 執行動作：在環境中執行動作。 獲得獎勵：環境給予獎勵。 更新知識：更新價值函數或Q函數，以改善未來的決策。 常見算法 Q-learning：使用Q函數學習最佳行動。 SARSA：使用價值函數學習最佳行動。 Deep Q-Networks (DQN)：使用深度神經網絡來近似Q函數。 Policy Gradient Methods：直接學習策略而非價值函數。 Value Function VS Q-Function 在強化學習（Reinforcement Learning, RL）中，Value Function（價值函數） 和 Q-Function（Q 值函數） 都是用來評估策略 $\pi$ 的好壞，但它們的側重點不同。
函數 定義 描述 State Value Function（狀態價值函數）$V(s)$ $V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, \pi \right]$ 表示 在狀態 $s$ 下，根據策略 $\pi$ 所能期望獲得的累積回報。 Action-Value Function（行動價值函數）$Q(s, a)$ $Q^\pi(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, A_0 = a, \pi \right]$ 表示 在狀態 $s$ 下執行動作 $a$，並按照策略 $\pi$ 行動後，所能期望獲得的累積回報。 Value Function $$ V^\pi(s) = \mathbb{E}\pi \left[ \sum{t=0}^\infty \gamma^t r_t \mid s_0 = s \right] $$"><meta property="og:url" content="https://bernie6401.github.io/terminology/reinforcement-learning/index.html"><meta property="og:site_name" content="SBK Hugo Site"><meta property="og:title" content="Reinforcement Learning :: SBK Hugo Site"><meta property="og:description" content="Reinforcement Learning 什麼是RL Reinforcement Learning（強化學習）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為。以下是強化學習的幾個應用面向，以及一些相關論文的參考： 遊戲：強化學習在遊戲領域中取得了顯著成就，例如AlphaGo和AlphaZero等。 機器人控制：用於控制機器人完成複雜任務，如行走、抓取物體等。 股票預測：應用於股票交易、投資策略等金融領域。 交通：用於優化交通信號控制、路線規劃等。
基本概念 強化學習（Reinforcement Learning）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為:
代理（Agent）：在環境中採取行動的實體，可以是機器人、軟件程序等。 環境（Environment）：代理所處的外部世界，提供狀態和反饋給代理。 狀態（State）：環境在某一時刻的描述，通常用$s$ 表示。 行動（Action）：代理在環境中採取的動作，通常用$a$ 表示。 獎勵（Reward）：環境對代理行動的反饋，用於引導學習，通常用$r$ 表示。 策略（Policy）：代理根據狀態選擇行動的方法，通常用$\pi(a|s)$ 表示。 價值函數（Value Function）：評估在某一狀態下遵循特定策略的預期累積獎勵，用於評估狀態的好壞。 Q函數（Q-Function）：評估在某一狀態下採取特定行動後遵循特定策略的預期累積獎勵。 基本過程 初始化：代理開始與環境交互。 觀察狀態：代理觀察環境的當前狀態。 選擇行動：根據策略選擇行動。 執行動作：在環境中執行動作。 獲得獎勵：環境給予獎勵。 更新知識：更新價值函數或Q函數，以改善未來的決策。 常見算法 Q-learning：使用Q函數學習最佳行動。 SARSA：使用價值函數學習最佳行動。 Deep Q-Networks (DQN)：使用深度神經網絡來近似Q函數。 Policy Gradient Methods：直接學習策略而非價值函數。 Value Function VS Q-Function 在強化學習（Reinforcement Learning, RL）中，Value Function（價值函數） 和 Q-Function（Q 值函數） 都是用來評估策略 $\pi$ 的好壞，但它們的側重點不同。
函數 定義 描述 State Value Function（狀態價值函數）$V(s)$ $V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, \pi \right]$ 表示 在狀態 $s$ 下，根據策略 $\pi$ 所能期望獲得的累積回報。 Action-Value Function（行動價值函數）$Q(s, a)$ $Q^\pi(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, A_0 = a, \pi \right]$ 表示 在狀態 $s$ 下執行動作 $a$，並按照策略 $\pi$ 行動後，所能期望獲得的累積回報。 Value Function $$ V^\pi(s) = \mathbb{E}\pi \left[ \sum{t=0}^\infty \gamma^t r_t \mid s_0 = s \right] $$"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="Terminologies"><meta property="article:tag" content="名詞解釋"><meta itemprop=name content="Reinforcement Learning :: SBK Hugo Site"><meta itemprop=description content="Reinforcement Learning 什麼是RL Reinforcement Learning（強化學習）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為。以下是強化學習的幾個應用面向，以及一些相關論文的參考： 遊戲：強化學習在遊戲領域中取得了顯著成就，例如AlphaGo和AlphaZero等。 機器人控制：用於控制機器人完成複雜任務，如行走、抓取物體等。 股票預測：應用於股票交易、投資策略等金融領域。 交通：用於優化交通信號控制、路線規劃等。
基本概念 強化學習（Reinforcement Learning）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為:
代理（Agent）：在環境中採取行動的實體，可以是機器人、軟件程序等。 環境（Environment）：代理所處的外部世界，提供狀態和反饋給代理。 狀態（State）：環境在某一時刻的描述，通常用$s$ 表示。 行動（Action）：代理在環境中採取的動作，通常用$a$ 表示。 獎勵（Reward）：環境對代理行動的反饋，用於引導學習，通常用$r$ 表示。 策略（Policy）：代理根據狀態選擇行動的方法，通常用$\pi(a|s)$ 表示。 價值函數（Value Function）：評估在某一狀態下遵循特定策略的預期累積獎勵，用於評估狀態的好壞。 Q函數（Q-Function）：評估在某一狀態下採取特定行動後遵循特定策略的預期累積獎勵。 基本過程 初始化：代理開始與環境交互。 觀察狀態：代理觀察環境的當前狀態。 選擇行動：根據策略選擇行動。 執行動作：在環境中執行動作。 獲得獎勵：環境給予獎勵。 更新知識：更新價值函數或Q函數，以改善未來的決策。 常見算法 Q-learning：使用Q函數學習最佳行動。 SARSA：使用價值函數學習最佳行動。 Deep Q-Networks (DQN)：使用深度神經網絡來近似Q函數。 Policy Gradient Methods：直接學習策略而非價值函數。 Value Function VS Q-Function 在強化學習（Reinforcement Learning, RL）中，Value Function（價值函數） 和 Q-Function（Q 值函數） 都是用來評估策略 $\pi$ 的好壞，但它們的側重點不同。
函數 定義 描述 State Value Function（狀態價值函數）$V(s)$ $V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, \pi \right]$ 表示 在狀態 $s$ 下，根據策略 $\pi$ 所能期望獲得的累積回報。 Action-Value Function（行動價值函數）$Q(s, a)$ $Q^\pi(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, A_0 = a, \pi \right]$ 表示 在狀態 $s$ 下執行動作 $a$，並按照策略 $\pi$ 行動後，所能期望獲得的累積回報。 Value Function $$ V^\pi(s) = \mathbb{E}\pi \left[ \sum{t=0}^\infty \gamma^t r_t \mid s_0 = s \right] $$"><meta itemprop=wordCount content="634"><meta itemprop=keywords content="名詞解釋"><title>Reinforcement Learning :: SBK Hugo Site</title>
<link href=/fonts/fontawesome/css/fontawesome-all.min.css?1743619851 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/fonts/fontawesome/css/fontawesome-all.min.css?1743619851 rel=stylesheet></noscript><link href=/css/perfect-scrollbar/perfect-scrollbar.min.css?1743619851 rel=stylesheet><link href=/css/theme.min.css?1743619851 rel=stylesheet><link href=/css/format-html.min.css?1743619851 rel=stylesheet id=R-format-style><link href=/css/auto-complete/auto-complete.min.css?1743619851 rel=stylesheet><script src=/js/auto-complete/auto-complete.min.js?1743619851 defer></script><script src=/js/lunr/lunr.min.js?1743619851 defer></script><script src=/js/lunr/lunr.stemmer.support.min.js?1743619851 defer></script><script src=/js/lunr/lunr.multi.min.js?1743619851 defer></script><script src=/js/lunr/lunr.en.min.js?1743619851 defer></script><script src=/js/search.min.js?1743619851 defer></script><script>window.relearn=window.relearn||{},window.relearn.min=`.min`,window.relearn.path="/terminology/reinforcement-learning/index.html",window.relearn.relBasePath="../..",window.relearn.relBaseUri="../..",window.relearn.absBaseUri="https://bernie6401.github.io",window.relearn.contentLangs=["en"],window.relearn.index_js_url="/searchindex.en.js?1743619851",window.relearn.disableAnchorCopy=!1,window.relearn.disableAnchorScrolling=!1,window.relearn.disableInlineCopyToClipboard=!1,window.relearn.enableBlockCodeWrap=!0,window.relearn.getItem=(e,t)=>e.getItem(t),window.relearn.setItem=(e,t,n)=>e.setItem(t,n),window.relearn.removeItem=(e,t)=>e.removeItem(t),window.relearn.themevariants=["auto"],window.relearn.customvariantname="my-custom-variant",window.relearn.changeVariant=function(e){var t=document.documentElement.dataset.rThemeVariant;window.relearn.setItem(window.localStorage,window.relearn.absBaseUri+"/variant",e),document.documentElement.dataset.rThemeVariant=e,t!=e&&(document.dispatchEvent(new CustomEvent("themeVariantLoaded",{detail:{variant:e,oldVariant:t}})),window.relearn.markVariant())},window.relearn.markVariant=function(){var e=window.relearn.getItem(window.localStorage,window.relearn.absBaseUri+"/variant");document.querySelectorAll(".R-variantswitcher select").forEach(t=>{t.value=e})},window.relearn.initVariant=function(){var e=window.relearn.getItem(window.localStorage,window.relearn.absBaseUri+"/variant")??"";e==window.relearn.customvariantname||(!e||!window.relearn.themevariants.includes(e))&&(e=window.relearn.themevariants[0],window.relearn.setItem(window.localStorage,window.relearn.absBaseUri+"/variant",e)),document.documentElement.dataset.rThemeVariant=e},window.relearn.initVariant(),window.relearn.markVariant(),window.T_Copy_to_clipboard=`Copy to clipboard`,window.T_Copied_to_clipboard=`Copied to clipboard!`,window.T_Copy_link_to_clipboard=`Copy link to clipboard`,window.T_Link_copied_to_clipboard=`Copied link to clipboard!`,window.T_Reset_view=`Reset view`,window.T_View_reset=`View reset!`,window.T_No_results_found=`No results found for "{0}"`,window.T_N_results_found=`{1} results found for "{0}"`</script></head><body class="mobile-support html" data-url=/terminology/reinforcement-learning/index.html><div id=R-body class=default-animation><div id=R-body-overlay></div><nav id=R-topbar><div class=topbar-wrapper><div class=topbar-sidebar-divider></div><div class="topbar-area topbar-area-start" data-area=start><div class="topbar-button topbar-button-sidebar" data-content-empty=disable data-width-s=show data-width-m=hide data-width-l=hide><button class=topbar-control onclick=toggleNav() type=button title="Menu (CTRL+ALT+n)"><i class="fa-fw fas fa-bars"></i></button></div><div class="topbar-button topbar-button-toc" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title="Table of Contents (CTRL+ALT+t)"><i class="fa-fw fas fa-list-alt"></i></button><div class=topbar-content><div class=topbar-content-wrapper><nav class=TableOfContents><ul><li><a href=#什麼是rl>什麼是RL</a></li><li><a href=#基本概念>基本概念</a><ul><li><a href=#基本過程>基本過程</a></li><li><a href=#常見算法>常見算法</a></li></ul></li><li><a href=#value-function-vs-q-function>Value Function VS Q-Function</a><ul><li><a href=#舉例自駕車停車><strong>舉例：自駕車停車</strong></a></li><li><a href=#總結><strong>總結</strong></a></li></ul></li><li><a href=#on-policy-vs-off-policy>On-Policy VS Off-Policy</a><ul><li><a href=#1-on-policy內部策略學習><strong>1. On-Policy（內部策略學習）</strong></a></li><li><a href=#2-off-policy外部策略學習><strong>2. Off-Policy（外部策略學習）</strong></a></li><li><a href=#3-直觀舉例><strong>3. 直觀舉例</strong></a></li><li><a href=#4-總結on-policy-vs-off-policy><strong>4. 總結：On-Policy vs. Off-Policy</strong></a></li></ul></li><li><a href=#what-is-sarsa--q-learning>What is SARSA & Q-Learning</a><ul><li><a href=#1-sarsaon-policy><strong>1. SARSA（On-Policy）</strong></a></li><li><a href=#2-q-learningoff-policy><strong>2. Q-Learning（Off-Policy）</strong></a></li><li><a href=#3-sarsa-vs-q-learning比較><strong>3. SARSA vs. Q-Learning（比較）</strong></a></li><li><a href=#4-何時用-sarsa何時用-q-learning><strong>4. 何時用 SARSA？何時用 Q-Learning？</strong></a></li><li><a href=#5-總結><strong>5. 總結</strong></a></li></ul></li></ul></nav></div></div></div></div><ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype=http://schema.org/BreadcrumbList><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/index.html><span itemprop=name>SBK Hugo Site</span></a><meta itemprop=position content="1">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><a itemprop=item href=/terminology/index.html><span itemprop=name>Terminologies</span></a><meta itemprop=position content="2">&nbsp;>&nbsp;</li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><span itemprop=name>Reinforcement Learning</span><meta itemprop=position content="3"></li></ol><div class="topbar-area topbar-area-end" data-area=end><div class="topbar-button topbar-button-prev" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><a class=topbar-control href=/terminology/programming-related/index.html title="Programming Related (🡐)"><i class="fa-fw fas fa-chevron-left"></i></a></div><div class="topbar-button topbar-button-next" data-content-empty=disable data-width-s=show data-width-m=show data-width-l=show><a class=topbar-control href=/terminology/what-is-devops-mlops-ci_cd_/index.html title="What is DevOps, MLOps, CI/CD? (🡒)"><i class="fa-fw fas fa-chevron-right"></i></a></div><div class="topbar-button topbar-button-more" data-content-empty=hide data-width-s=show data-width-m=show data-width-l=show><button class=topbar-control onclick=toggleTopbarFlyout(this) type=button title=More><i class="fa-fw fas fa-ellipsis-v"></i></button><div class=topbar-content><div class=topbar-content-wrapper><div class="topbar-area topbar-area-more" data-area=more></div></div></div></div></div></div></nav><div id=R-main-overlay></div><main id=R-body-inner class="highlightable terminology" tabindex=-1><div class=flex-block-wrapper><article class=default><header class=headline><div class="R-taxonomy taxonomy-tags cstyle tags" title=Tags style=--VARIABLE-TAGS-BG-color:var(--INTERNAL-TAG-BG-color)><ul><li><a class=term-link href=/tags/%E5%90%8D%E8%A9%9E%E8%A7%A3%E9%87%8B/index.html>名詞解釋</a></li></ul></div></header><h1 id=reinforcement-learning>Reinforcement Learning</h1><h1 id=reinforcement-learning>Reinforcement Learning</h1><h2 id=什麼是rl>什麼是RL</h2><p>Reinforcement Learning（強化學習）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為。以下是強化學習的幾個應用面向，以及一些相關論文的參考：
遊戲：強化學習在遊戲領域中取得了顯著成就，例如AlphaGo和AlphaZero等。
機器人控制：用於控制機器人完成複雜任務，如行走、抓取物體等。
股票預測：應用於股票交易、投資策略等金融領域。
交通：用於優化交通信號控制、路線規劃等。</p><h2 id=基本概念>基本概念</h2><p>強化學習（Reinforcement Learning）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為:</p><ol><li><strong>代理（Agent）</strong>：在環境中採取行動的實體，可以是機器人、軟件程序等。</li><li><strong>環境（Environment）</strong>：代理所處的外部世界，提供狀態和反饋給代理。</li><li><strong>狀態（State）</strong>：環境在某一時刻的描述，通常用$s$ 表示。</li><li><strong>行動（Action）</strong>：代理在環境中採取的動作，通常用$a$ 表示。</li><li><strong>獎勵（Reward）</strong>：環境對代理行動的反饋，用於引導學習，通常用$r$ 表示。</li><li><strong>策略（Policy）</strong>：代理根據狀態選擇行動的方法，通常用$\pi(a|s)$ 表示。</li><li><strong>價值函數（Value Function）</strong>：評估在某一狀態下遵循特定策略的預期累積獎勵，用於評估狀態的好壞。</li><li><strong>Q函數（Q-Function）</strong>：評估在某一狀態下採取特定行動後遵循特定策略的預期累積獎勵。</li></ol><h3 id=基本過程>基本過程</h3><ol><li><strong>初始化</strong>：代理開始與環境交互。</li><li><strong>觀察狀態</strong>：代理觀察環境的當前狀態。</li><li><strong>選擇行動</strong>：根據策略選擇行動。</li><li><strong>執行動作</strong>：在環境中執行動作。</li><li><strong>獲得獎勵</strong>：環境給予獎勵。</li><li><strong>更新知識</strong>：更新價值函數或Q函數，以改善未來的決策。</li></ol><h3 id=常見算法>常見算法</h3><ul><li><strong>Q-learning</strong>：使用Q函數學習最佳行動。</li><li><strong>SARSA</strong>：使用價值函數學習最佳行動。</li><li><strong>Deep Q-Networks (DQN)</strong>：使用深度神經網絡來近似Q函數。</li><li><strong>Policy Gradient Methods</strong>：直接學習策略而非價值函數。</li></ul><h2 id=value-function-vs-q-function>Value Function VS Q-Function</h2><p>在強化學習（Reinforcement Learning, RL）中，<strong>Value Function（價值函數）</strong> 和 <strong>Q-Function（Q 值函數）</strong> 都是用來評估策略 $\pi$ 的好壞，但它們的側重點不同。</p><table><thead><tr><th><strong>函數</strong></th><th><strong>定義</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td><strong>State Value Function（狀態價值函數）$V(s)$</strong></td><td>$V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, \pi \right]$</td><td>表示 <strong>在狀態 $s$ 下，根據策略 $\pi$ 所能期望獲得的累積回報</strong>。</td></tr><tr><td><strong>Action-Value Function（行動價值函數）$Q(s, a)$</strong></td><td>$Q^\pi(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, A_0 = a, \pi \right]$</td><td>表示 <strong>在狀態 $s$ 下執行動作 $a$，並按照策略 $\pi$ 行動後，所能期望獲得的累積回報</strong>。</td></tr></tbody></table><ul><li><p>Value Function
$$
V^\pi(s) = \mathbb{E}<em>\pi \left[ \sum</em>{t=0}^\infty \gamma^t r_t \mid s_0 = s \right]
$$</p><p>其中：</p><ul><li>$\mathbb{E}_\pi$ 表示在策略$\pi$下取期望。</li><li>$r_t$ 是在時間步$t$獲得的獎勵。</li><li>$\gamma$ 是折扣因子，控制未來獎勵的重要性。</li><li>$s_0 = s$ 表示初始狀態為$s$。</li></ul></li><li><p>Q-Function
$$
Q^\pi(s, a) = \mathbb{E}_\pi \left[ r + \gamma V^\pi(s&rsquo;) \mid s, a \right]
$$</p><p>或更一般地：</p><p>$$
Q^\pi(s, a) = \mathbb{E}<em>\pi \left[ \sum</em>{t=0}^\infty \gamma^t r_t \mid s_0 = s, a_0 = a \right]
$$</p><p>其中：</p><ul><li>$s&rsquo;$ 是採取行動$a$後的下一狀態。</li><li>其他符號與價值函數的表達式中相同。</li></ul><p>在Q-learning等算法中，Q函數通常使用以下更新規則來學習：</p><p>$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a&rsquo;} Q(s&rsquo;, a&rsquo;) - Q(s, a) \right]
$$</p><p>其中：</p><ul><li>$\alpha$ 是學習率。</li><li>$\max_{a&rsquo;} Q(s&rsquo;, a&rsquo;)$ 是下一狀態中所有可能行動的最大Q值。</li></ul></li></ul><ul><li><strong>$V(s)$ 只考慮當前狀態的價值，適合評估當前策略的整體表現。</strong></li><li><strong>$Q(s, a)$ 則考慮特定動作的價值，適合用於決策選擇最佳動作（policy improvement）。</strong></li></ul><hr><h3 id=舉例自駕車停車><strong>舉例：自駕車停車</strong></h3><p>假設我們在訓練一輛 <strong>自動駕駛車</strong> 停進停車位的策略。</p><h4 id=1-狀態價值函數-vs><strong>1. 狀態價值函數 $V(s)$</strong></h4><p>假設 <strong>狀態 $s$</strong> 表示車輛目前的位置，我們有：</p><ul><li>$V(s_1) = 0.8$ 代表在停車場入口時，預計可以獲得 0.8 的回報（可能是停好車的成功率）。</li><li>$V(s_2) = 0.9$ 代表在更接近停車位的位置時，回報變高。</li></ul><p>這表示 <strong>越接近停車位的狀態，價值越高</strong>，但這裡沒有考慮具體的行動選擇。</p><h4 id=2-q-值函數-qs-a><strong>2. Q 值函數 $Q(s, a)$</strong></h4><p>假設 <strong>動作 $a$</strong> 包括：</p><ul><li><strong>$a_1$ = 前進</strong>（move forward）</li><li><strong>$a_2$ = 左轉</strong>（turn left）</li><li><strong>$a_3$ = 右轉</strong>（turn right）</li></ul><p>我們可以有：</p><ul><li>$Q(s_1, a_1) = 0.6$（在入口時，選擇前進的預期回報是 0.6）</li><li>$Q(s_1, a_2) = 0.2$（在入口時，直接左轉可能導致撞牆，所以回報較低）</li><li>$Q(s_2, a_1) = 0.9$（更接近停車位時選擇前進，回報較高）</li></ul><p>這表示 <strong>Q 值函數不僅考慮當前狀態，還考慮具體的行動對未來回報的影響</strong>。</p><hr><h3 id=總結><strong>總結</strong></h3><ul><li><strong>$V(s)$ 只告訴我們當前狀態好不好，但不告訴我們該做什麼動作。</strong></li><li><strong>$Q(s, a)$ 告訴我們在當前狀態 $s$ 下，選擇不同動作 $a$ 的好壞，可以用來決策選擇最好的動作（如 Q-Learning）。</strong></li></ul><p>🚗 <strong>自駕車例子：</strong></p><ul><li><strong>$V(s)$</strong> 只是說「這個位置靠近停車位，所以好」，但 <strong>不告訴我們該轉向還是前進</strong>。</li><li><strong>$Q(s, a)$</strong> 具體說明「這裡左轉不好，前進較好」，幫助我們選擇最佳動作。</li></ul><p>在 <strong>Q-Learning</strong> 和 <strong>Deep Q-Networks（DQN）</strong> 等方法中，<strong>主要學習 $Q(s, a)$，然後選擇最大 Q 值的動作來更新策略</strong>。</p><h2 id=on-policy-vs-off-policy>On-Policy VS Off-Policy</h2><p>在強化學習（Reinforcement Learning, RL）中，<strong>On-Policy</strong> 和 <strong>Off-Policy</strong> 的區別主要在於<strong>學習時使用的策略（policy）與執行時的策略是否相同</strong>。</p><p>如果用一句話概括：</p><ul><li><strong>On-Policy（內部策略學習）</strong>：學習與執行同一個策略。</li><li><strong>Off-Policy（外部策略學習）</strong>：學習時使用與執行不同的策略。</li></ul><hr><h3 id=1-on-policy內部策略學習><strong>1. On-Policy（內部策略學習）</strong></h3><h4 id=定義><strong>定義</strong></h4><ul><li><strong>在收集數據時，使用的策略（行動選擇）與學習時的策略相同。</strong></li><li>也就是說，演算法只能學習當前策略 $\pi$，並依賴 $\pi$ 產生的經驗來改進自身。</li></ul><h4 id=代表性演算法><strong>代表性演算法</strong></h4><ul><li><strong>SARSA</strong>（State-Action-Reward-State-Action）<ul><li>依據當前策略 $\pi$ 來選擇動作並更新 Q 值。</li><li>例如，如果使用 $\epsilon$-貪婪策略（$\epsilon$-greedy），則學習的 Q 值也會考慮這種策略下的行動。</li></ul></li></ul><h4 id=特點><strong>特點</strong></h4><p>✅ <strong>適合策略改進（policy improvement）</strong>，因為它直接學習當前策略的行為。
✅ <strong>收斂性較穩定</strong>，因為學到的價值估計與執行行為相匹配。
❌ <strong>探索能力有限</strong>，因為只能學習自己當前策略的數據，難以學習更好的行動。</p><hr><h3 id=2-off-policy外部策略學習><strong>2. Off-Policy（外部策略學習）</strong></h3><h4 id=定義-1><strong>定義</strong></h4><ul><li><strong>學習時的策略與執行時的策略不同</strong>，即可以用<strong>不同的策略來收集數據</strong>，然後用這些數據來學習更好的策略。</li><li>這允許模型透過<strong>試探性策略（exploration policy）</strong> 來收集數據，但學習一個更優的<strong>目標策略（target policy）</strong>。</li></ul><h4 id=代表性演算法-1><strong>代表性演算法</strong></h4><ul><li><p><strong>Q-Learning</strong></p><ul><li>無論探索時是否選擇了最佳動作，更新 Q 值時都<strong>假設每個狀態都會選擇最優動作（max Q）</strong>，這使得它可以學習最優策略。</li></ul></li><li><p><strong>Deep Q-Networks（DQN）</strong></p><ul><li>使用 <strong>經驗回放（experience replay）</strong>，存儲過去的數據並從中抽樣來訓練 Q 網絡，使得學習與數據收集分離，這本質上是一種 Off-Policy 方法。</li></ul></li></ul><h4 id=特點-1><strong>特點</strong></h4><p>✅ <strong>探索能力更強</strong>，因為可以使用不同的策略來收集更多多樣的數據。
✅ <strong>可以利用過去經驗數據</strong>，例如 DQN 的<strong>經驗回放（experience replay）</strong>。
❌ <strong>收斂可能不穩定</strong>，因為學習的策略與數據來源可能不匹配。</p><hr><h3 id=3-直觀舉例><strong>3. 直觀舉例</strong></h3><h4 id=例子-1自駕車><strong>例子 1：自駕車</strong></h4><p>假設你在訓練<strong>自動駕駛車輛</strong>，你的目標是讓車學習到最安全的駕駛方式：</p><ul><li><p><strong>On-Policy（SARSA）</strong>：</p><ul><li>車輛根據當前駕駛策略行駛，然後學習基於這種駕駛風格的價值函數。例如，如果車輛<strong>偶爾</strong>違規變道，學習到的策略仍然會保留這種行為。</li><li>缺點是如果當前策略不夠優秀，學到的內容可能也不夠優秀。</li></ul></li><li><p><strong>Off-Policy（Q-Learning）</strong>：</p><ul><li>你讓不同車輛<strong>嘗試各種駕駛風格</strong>（包括安全駕駛和激進駕駛），然後學習<strong>最安全的駕駛策略</strong>。</li><li>這樣，即使有些車輛的行為不理想，演算法仍能學習到更好的策略。</li></ul></li></ul><hr><h4 id=例子-2小朋友學騎腳踏車><strong>例子 2：小朋友學騎腳踏車</strong></h4><ul><li><p><strong>On-Policy（SARSA）</strong>：</p><ul><li>小朋友根據自己當前的學習方式練習，例如<strong>只嘗試自己能做的動作</strong>，然後根據這些嘗試來調整策略。</li><li>如果他害怕摔倒，不會嘗試太多冒險的動作，因此學習速度可能較慢。</li></ul></li><li><p><strong>Off-Policy（Q-Learning）</strong>：</p><ul><li>小朋友可以<strong>看別人騎腳踏車</strong>，學習到最好的技巧，然後用來改善自己的策略。</li><li>這樣，即使自己還沒嘗試某些行動，也可以學習到這些行為的價值。</li></ul></li></ul><hr><h3 id=4-總結on-policy-vs-off-policy><strong>4. 總結：On-Policy vs. Off-Policy</strong></h3><table><thead><tr><th><strong>比較項目</strong></th><th><strong>On-Policy（內部策略）</strong></th><th><strong>Off-Policy（外部策略）</strong></th></tr></thead><tbody><tr><td><strong>學習策略</strong></td><td>只能學習自身策略的數據</td><td>可以從不同策略的數據中學習</td></tr><tr><td><strong>探索能力</strong></td><td>受限於當前策略的行為</td><td>可以透過不同策略收集更多數據</td></tr><tr><td><strong>代表演算法</strong></td><td>SARSA, REINFORCE</td><td>Q-Learning, DQN</td></tr><tr><td><strong>應用場景</strong></td><td>策略梯度（Policy Gradient）、安全性較高的應用</td><td>需要長期探索、大量數據的應用</td></tr><tr><td><strong>穩定性</strong></td><td>通常比較穩定</td><td>可能會收斂較慢或不穩定</td></tr></tbody></table><p><strong>結論：</strong></p><ul><li><strong>On-Policy（如 SARSA）適合需要安全性與穩定性的學習場景</strong>，但可能學不到最優策略。</li><li><strong>Off-Policy（如 Q-Learning）可以利用更多數據來學習更好的策略</strong>，但可能會不穩定或過度偏向最大 Q 值的選擇。</li></ul><p>如果你想讓模型學到<strong>最好的策略（最優解）</strong>，通常會選擇 <strong>Off-Policy（如 Q-Learning）</strong>。如果你想要學習穩定且不會意外做出危險決策的策略，則會選擇 <strong>On-Policy（如 SARSA）</strong>。</p><h2 id=what-is-sarsa--q-learning>What is SARSA & Q-Learning</h2><p>:::info
詳細數學推導可以參考: <a href=https://hackmd.io/@RL666/SkNJdfhn9 rel=external target=_blank>深度強化學習 Ch3.1 : TD learning</a>
:::
<strong>SARSA（State-Action-Reward-State-Action）</strong> 和 <strong>Q-Learning</strong> 都是強化學習（Reinforcement Learning, RL）中的<strong>值基方法（Value-Based Methods）</strong>，它們的主要目標是<strong>學習動作-價值函數（Q-Function）</strong>，以選擇最佳動作來最大化累積回報。</p><p>它們的核心區別在於<strong>策略選擇方式</strong>：</p><ul><li><strong>SARSA 是 On-Policy（內部策略學習）</strong>，學習當前策略的行為。</li><li><strong>Q-Learning 是 Off-Policy（外部策略學習）</strong>，學習最優策略，即使數據來自不同的行為策略。</li></ul><hr><h3 id=1-sarsaon-policy><strong>1. SARSA（On-Policy）</strong></h3><h4 id=核心概念><strong>核心概念</strong></h4><p>SARSA 是<strong>On-Policy</strong> 強化學習方法，意味著<strong>它學習的是當前正在執行的策略（policy）</strong>，即它的 Q 值更新是基於實際選擇的動作，而不是理論上的最優動作。</p><h4 id=更新公式><strong>更新公式</strong></h4><p>$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
$$
其中：</p><ul><li>$s_t, a_t, r_t$：當前狀態、動作和獎勵。</li><li>$s_{t+1}, a_{t+1}$：下一個狀態及根據當前策略選擇的下一個動作。</li><li>$\alpha$（學習率）：控制更新步伐。</li><li>$\gamma$（折扣因子）：決定未來回報的重要性。</li></ul><h4 id=sarsa-的特點><strong>SARSA 的特點</strong></h4><p>✅ <strong>學習當前策略的行為</strong>，不會過於激進地選擇最優解，較穩定。
✅ <strong>策略與學習方式一致</strong>，不會突然變成極端的貪婪策略，適合安全性要求高的環境。
❌ <strong>可能學不到最優策略</strong>，如果策略不是最優的，那學習結果也可能不是最優的。</p><h4 id=舉例><strong>舉例</strong></h4><p><strong>🛵 例子：學騎腳踏車</strong></p><ul><li>小朋友用 SARSA 來學習騎腳踏車，他會根據自己的學習風格來選擇動作（如小心慢騎）。</li><li>當他遇到坑洞時，他會根據自己當前的策略決定是慢慢避開還是稍微加速躲開。</li><li>這代表他學習的是一種「安全但可能不是最快的騎車方式」。</li></ul><hr><h3 id=2-q-learningoff-policy><strong>2. Q-Learning（Off-Policy）</strong></h3><h4 id=核心概念-1><strong>核心概念</strong></h4><p>Q-Learning 是<strong>Off-Policy</strong> 強化學習方法，這意味著它<strong>學習的是最優策略，而不管數據來自哪種策略</strong>。</p><ul><li>即使當前的行為策略（例如隨機選擇動作）不是最優的，Q-Learning 依然會學習最優的行動選擇方式。</li></ul><h4 id=更新公式-1><strong>更新公式</strong></h4><p>$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \right]
$$
與 SARSA 唯一的區別是：</p><ul><li>SARSA 用的是「<strong>實際選擇的下一個動作</strong>」$Q(s_{t+1}, a_{t+1})$。</li><li>Q-Learning 直接選擇「<strong>下一個狀態的最優動作</strong>」$\max_{a} Q(s_{t+1}, a)$ 來更新 Q 值。</li></ul><h4 id=q-learning-的特點><strong>Q-Learning 的特點</strong></h4><p>✅ <strong>能學到最優策略</strong>，因為它總是選擇最好的 Q 值來學習。
✅ <strong>可以利用 Off-Policy 數據</strong>，例如過去的經驗來更新學習結果（像 DQN 的經驗回放）。
❌ <strong>可能學習不穩定</strong>，如果環境變化大，它可能會過度偏向最優動作而忽略探索。</p><h4 id=舉例-1><strong>舉例</strong></h4><p><strong>🚗 例子：自駕車找停車位</strong></p><ul><li>假設 Q-Learning 訓練一輛<strong>自動駕駛車</strong>來找停車位。</li><li>雖然車輛一開始可能亂選路線，但最終它會學到「<strong>最短最快的停車路線</strong>」。</li><li>即使一開始的數據來自於人類駕駛或隨機行為，Q-Learning 仍然會學習出<strong>最優的停車策略</strong>。</li></ul><hr><h3 id=3-sarsa-vs-q-learning比較><strong>3. SARSA vs. Q-Learning（比較）</strong></h3><table><thead><tr><th><strong>比較項目</strong></th><th><strong>SARSA（On-Policy）</strong></th><th><strong>Q-Learning（Off-Policy）</strong></th></tr></thead><tbody><tr><td><strong>學習策略</strong></td><td>學習當前執行的策略</td><td>學習最優策略</td></tr><tr><td><strong>更新方式</strong></td><td>依據實際選擇的行動更新</td><td>依據最大 Q 值更新</td></tr><tr><td><strong>穩定性</strong></td><td>收斂較穩定，但不一定最優</td><td>可能不穩定，但學習結果更好</td></tr><tr><td><strong>探索能力</strong></td><td>受限於當前策略的行為</td><td>可以從不同策略的數據中學習</td></tr><tr><td><strong>代表性應用</strong></td><td>需要安全性高的應用，如醫療機器人</td><td>需要找到最優解的應用，如自駕車</td></tr><tr><td><strong>適用場景</strong></td><td>策略梯度、保守決策環境</td><td>需要大量探索、目標最優化的環境</td></tr></tbody></table><h4 id=核心區別><strong>核心區別</strong></h4><ol><li><strong>SARSA 是 On-Policy，學習的是「當前策略」，適合安全性較高的情境（如自駕車減速避讓行人）。</strong></li><li><strong>Q-Learning 是 Off-Policy，學習的是「最優策略」，適合追求最優解的環境（如最快找到停車位）。</strong></li></ol><hr><h3 id=4-何時用-sarsa何時用-q-learning><strong>4. 何時用 SARSA？何時用 Q-Learning？</strong></h3><p>✅ <strong>SARSA（On-Policy）適用情境</strong></p><ul><li>需要安全性較高的場景，例如：<ul><li><strong>醫療機器人</strong>（避免風險操作）</li><li><strong>自駕車避障</strong>（減少風險行為）</li><li><strong>教育 AI</strong>（確保不過於極端行為）</li></ul></li><li>主要用於「<strong>學習當前策略的行為</strong>」。</li></ul><p>✅ <strong>Q-Learning（Off-Policy）適用情境</strong></p><ul><li>需要找到全局最優解，例如：<ul><li><strong>遊戲 AI</strong>（學到最佳策略）</li><li><strong>自駕車尋找最佳路徑</strong>（找最短停車路線）</li><li><strong>金融交易策略</strong>（學到最賺錢的投資方式）</li></ul></li><li>主要用於「<strong>學習最優策略，而不管數據來源</strong>」。</li></ul><hr><h3 id=5-總結><strong>5. 總結</strong></h3><ul><li><strong>SARSA：On-Policy，學當前策略，較安全但可能不是最優解。</strong></li><li><strong>Q-Learning：Off-Policy，學最優策略，探索能力強但可能學習不穩定。</strong></li><li><strong>如果你希望 AI 探索最優解，選 Q-Learning；如果你希望 AI 行為穩定，選 SARSA。</strong></li></ul><footer class=footline></footer></article></div></main></div><aside id=R-sidebar class=default-animation><div id=R-header-topbar class=default-animation></div><div id=R-header-wrapper class=default-animation><div id=R-header class=default-animation><a id=R-logo class=R-default href=/index.html><div class=logo-title>SBK Hugo Site</div></a></div><search><form action=/search/index.html method=get><div class="searchbox default-animation"><button class=search-detail type=submit title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
<label class=a11y-only for=R-search-by>Search</label>
<input data-search-input id=R-search-by name=search-by class=search-by type=search placeholder=Search...>
<button class=search-clear type=button data-search-clear title="Clear search"><i class="fas fa-times" title="Clear search"></i></button></div></form></search></div><div id=R-homelinks class="default-animation homelinks"><div class="R-menu-divider default-animation"><hr class=padding></div><div class="R-sidebarmenu R-shortcutmenu-homelinks"><ul class="space collapsible-menu"><li data-nav-id=/index.html><a class=padding href=/index.html><i class="fa-fw fas fa-home"></i> Home</a></li></ul></div><div class="R-menu-divider default-animation"><hr class=padding></div><div class="R-sidebarmenu R-shortcutmenu-headercontrols"><ul></ul></div><div class="R-menu-divider default-animation"><hr class=padding></div></div><div id=R-content-wrapper class=highlightable><div class="R-sidebarmenu R-shortcutmenu-main"><ul class="enlarge morespace collapsible-menu"><li data-nav-id=/books-notes/index.html><a class=padding href=/books-notes/index.html>Books Notes</a><ul id=R-subsections-0771445a148f63eb6cc1a9b7ad45a36a class=collapsible-menu></ul></li><li data-nav-id=/data-structure/index.html><a class=padding href=/data-structure/index.html>Data Structures</a><ul id=R-subsections-58892abc7cea06bf1839098c82b3eb4d class=collapsible-menu></ul></li><li data-nav-id=/job/index.html><a class=padding href=/job/index.html>Jobs</a><ul id=R-subsections-0fce2c1f4d6d23f49b605855a82f55aa class=collapsible-menu></ul></li><li data-nav-id=/knowledge/index.html><a class=padding href=/knowledge/index.html>Knowledges</a><ul id=R-subsections-0bdb754d722e03f187f2117c1ef8015d class=collapsible-menu></ul></li><li data-nav-id=/leetcode/index.html><a class=padding href=/leetcode/index.html>LeetCodes</a><ul id=R-subsections-76e85062179d09303530c47eb73fc667 class=collapsible-menu></ul></li><li data-nav-id=/problem-solutions/index.html><a class=padding href=/problem-solutions/index.html>Problem Solutions</a><ul id=R-subsections-4f9e0f3095337026c1723515d5409f89 class=collapsible-menu></ul></li><li data-nav-id=/security/index.html><a class=padding href=/security/index.html>Securities</a><ul id=R-subsections-66815ecaaecfc1c209e5637d03b258b2 class=collapsible-menu></ul></li><li data-nav-id=/side-project/index.html><a class=padding href=/side-project/index.html>Side Projects</a><ul id=R-subsections-3295de6e89859c4cf23a4292e734c881 class=collapsible-menu></ul></li><li data-nav-id=/survey-papers/index.html><a class=padding href=/survey-papers/index.html>Survey Papers</a><ul id=R-subsections-baa5a308e222e800d6e93248dad73a11 class=collapsible-menu></ul></li><li class=parent data-nav-id=/terminology/index.html><a class=padding href=/terminology/index.html>Terminologies</a><ul id=R-subsections-a19bf87a9a9aaf0a4146b803a35492f8 class=collapsible-menu><li data-nav-id=/terminology/android-related/index.html><a class=padding href=/terminology/android-related/index.html>Android Related</a></li><li data-nav-id=/terminology/linux-related/index.html><a class=padding href=/terminology/linux-related/index.html>Linux Related</a></li><li data-nav-id=/terminology/math-related/index.html><a class=padding href=/terminology/math-related/index.html>Math Related</a></li><li data-nav-id=/terminology/network-related/index.html><a class=padding href=/terminology/network-related/index.html>Network Related</a></li><li data-nav-id=/terminology/node-related/index.html><a class=padding href=/terminology/node-related/index.html>Node Related</a></li><li data-nav-id=/terminology/programming-related/index.html><a class=padding href=/terminology/programming-related/index.html>Programming Related</a></li><li class=active data-nav-id=/terminology/reinforcement-learning/index.html><a class=padding href=/terminology/reinforcement-learning/index.html>Reinforcement Learning</a></li><li data-nav-id=/terminology/what-is-devops-mlops-ci_cd_/index.html><a class=padding href=/terminology/what-is-devops-mlops-ci_cd_/index.html>What is DevOps, MLOps, CI/CD?</a></li><li data-nav-id=/terminology/what-is-ids-ips-edr-mdr-nsm-siem_/index.html><a class=padding href=/terminology/what-is-ids-ips-edr-mdr-nsm-siem_/index.html>What is IDS, IPS, EDR, MDR, NSM, SIEM?</a></li></ul></li><li data-nav-id=/toc/index.html><a class=padding href=/toc/index.html>TOCs</a><ul id=R-subsections-451d3779340ac1afa8683c0232808cbf class=collapsible-menu></ul></li><li data-nav-id=/tools/index.html><a class=padding href=/tools/index.html>Tools</a><ul id=R-subsections-bf1399820dde3dd110e03ace4147ff86 class=collapsible-menu></ul></li></ul></div><div class="R-sidebarmenu R-shortcutmenu-shortcuts"><ul class="space collapsible-menu"></ul></div><div id=R-footer-margin></div><div class="R-menu-divider default-animation"><hr class=padding></div><div class="R-sidebarmenu R-shortcutmenu-footercontrols"><ul></ul></div><div id=R-footer><p>Built with <a href=https://github.com/McShelby/hugo-theme-relearn title=love><i class="fas fa-heart"></i></a> by <a href=https://gohugo.io/>Hugo</a></p></div></div></aside><script src=/js/clipboard/clipboard.min.js?1743619851 defer></script><script src=/js/perfect-scrollbar/perfect-scrollbar.min.js?1743619851 defer></script><script src=/js/theme.min.js?1743619851 defer></script></body></html>