---
title: Reinforcement Learning
tags: [名詞解釋]

category: "Terminology"
---

# Reinforcement Learning

## 什麼是RL
Reinforcement Learning（強化學習）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為。以下是強化學習的幾個應用面向，以及一些相關論文的參考：
遊戲：強化學習在遊戲領域中取得了顯著成就，例如AlphaGo和AlphaZero等。
機器人控制：用於控制機器人完成複雜任務，如行走、抓取物體等。
股票預測：應用於股票交易、投資策略等金融領域。
交通：用於優化交通信號控制、路線規劃等。

## 基本概念
強化學習（Reinforcement Learning）是一種機器學習方法，通過試錯和獎勵機制來學習最佳行為:
1. **代理（Agent）**：在環境中採取行動的實體，可以是機器人、軟件程序等。
2. **環境（Environment）**：代理所處的外部世界，提供狀態和反饋給代理。
3. **狀態（State）**：環境在某一時刻的描述，通常用$s$ 表示。
4. **行動（Action）**：代理在環境中採取的動作，通常用$a$ 表示。
5. **獎勵（Reward）**：環境對代理行動的反饋，用於引導學習，通常用$r$ 表示。
6. **策略（Policy）**：代理根據狀態選擇行動的方法，通常用$\pi(a|s)$ 表示。
7. **價值函數（Value Function）**：評估在某一狀態下遵循特定策略的預期累積獎勵，用於評估狀態的好壞。
8. **Q函數（Q-Function）**：評估在某一狀態下採取特定行動後遵循特定策略的預期累積獎勵。

### 基本過程
1. **初始化**：代理開始與環境交互。
2. **觀察狀態**：代理觀察環境的當前狀態。
3. **選擇行動**：根據策略選擇行動。
4. **執行動作**：在環境中執行動作。
5. **獲得獎勵**：環境給予獎勵。
6. **更新知識**：更新價值函數或Q函數，以改善未來的決策。

### 常見算法
- **Q-learning**：使用Q函數學習最佳行動。
- **SARSA**：使用價值函數學習最佳行動。
- **Deep Q-Networks (DQN)**：使用深度神經網絡來近似Q函數。
- **Policy Gradient Methods**：直接學習策略而非價值函數。

## Value Function VS Q-Function
在強化學習（Reinforcement Learning, RL）中，**Value Function（價值函數）** 和 **Q-Function（Q 值函數）** 都是用來評估策略 $\pi$ 的好壞，但它們的側重點不同。

| **函數** | **定義** | **描述** |
|----------|---------|----------|
| **State Value Function（狀態價值函數）$V(s)$** | $V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, \pi \right]$ | 表示 **在狀態 $s$ 下，根據策略 $\pi$ 所能期望獲得的累積回報**。 |
| **Action-Value Function（行動價值函數）$Q(s, a)$** | $Q^\pi(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid S_0 = s, A_0 = a, \pi \right]$ | 表示 **在狀態 $s$ 下執行動作 $a$，並按照策略 $\pi$ 行動後，所能期望獲得的累積回報**。 |
* Value Function
    $$
    V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s \right]
    $$

    其中：
    - $\mathbb{E}_\pi$ 表示在策略$\pi$下取期望。
    - $r_t$ 是在時間步$t$獲得的獎勵。
    - $\gamma$ 是折扣因子，控制未來獎勵的重要性。
    - $s_0 = s$ 表示初始狀態為$s$。
* Q-Function
    $$
    Q^\pi(s, a) = \mathbb{E}_\pi \left[ r + \gamma V^\pi(s') \mid s, a \right]
    $$

    或更一般地：

    $$
    Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s_0 = s, a_0 = a \right]
    $$

    其中：
    - $s'$ 是採取行動$a$後的下一狀態。
    - 其他符號與價值函數的表達式中相同。

    在Q-learning等算法中，Q函數通常使用以下更新規則來學習：

    $$
    Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
    $$

    其中：
    - $\alpha$ 是學習率。
    - $\max_{a'} Q(s', a')$ 是下一狀態中所有可能行動的最大Q值。

- **$V(s)$ 只考慮當前狀態的價值，適合評估當前策略的整體表現。**
- **$Q(s, a)$ 則考慮特定動作的價值，適合用於決策選擇最佳動作（policy improvement）。**

---

### **舉例：自駕車停車**
假設我們在訓練一輛 **自動駕駛車** 停進停車位的策略。

#### **1. 狀態價值函數 $V(s)$**
假設 **狀態 $s$** 表示車輛目前的位置，我們有：
- $V(s_1) = 0.8$ 代表在停車場入口時，預計可以獲得 0.8 的回報（可能是停好車的成功率）。
- $V(s_2) = 0.9$ 代表在更接近停車位的位置時，回報變高。

這表示 **越接近停車位的狀態，價值越高**，但這裡沒有考慮具體的行動選擇。

#### **2. Q 值函數 $Q(s, a)$**
假設 **動作 $a$** 包括：
- **$a_1$ = 前進**（move forward）
- **$a_2$ = 左轉**（turn left）
- **$a_3$ = 右轉**（turn right）

我們可以有：
- $Q(s_1, a_1) = 0.6$（在入口時，選擇前進的預期回報是 0.6）
- $Q(s_1, a_2) = 0.2$（在入口時，直接左轉可能導致撞牆，所以回報較低）
- $Q(s_2, a_1) = 0.9$（更接近停車位時選擇前進，回報較高）

這表示 **Q 值函數不僅考慮當前狀態，還考慮具體的行動對未來回報的影響**。

---

### **總結**
- **$V(s)$ 只告訴我們當前狀態好不好，但不告訴我們該做什麼動作。**
- **$Q(s, a)$ 告訴我們在當前狀態 $s$ 下，選擇不同動作 $a$ 的好壞，可以用來決策選擇最好的動作（如 Q-Learning）。**

🚗 **自駕車例子：**
- **$V(s)$** 只是說「這個位置靠近停車位，所以好」，但 **不告訴我們該轉向還是前進**。
- **$Q(s, a)$** 具體說明「這裡左轉不好，前進較好」，幫助我們選擇最佳動作。

在 **Q-Learning** 和 **Deep Q-Networks（DQN）** 等方法中，**主要學習 $Q(s, a)$，然後選擇最大 Q 值的動作來更新策略**。

## On-Policy VS Off-Policy
在強化學習（Reinforcement Learning, RL）中，**On-Policy** 和 **Off-Policy** 的區別主要在於**學習時使用的策略（policy）與執行時的策略是否相同**。

如果用一句話概括：
- **On-Policy（內部策略學習）**：學習與執行同一個策略。
- **Off-Policy（外部策略學習）**：學習時使用與執行不同的策略。

---

### **1. On-Policy（內部策略學習）**

#### **定義**
- **在收集數據時，使用的策略（行動選擇）與學習時的策略相同。**
- 也就是說，演算法只能學習當前策略 $\pi$，並依賴 $\pi$ 產生的經驗來改進自身。

#### **代表性演算法**
- **SARSA**（State-Action-Reward-State-Action）
  - 依據當前策略 $\pi$ 來選擇動作並更新 Q 值。
  - 例如，如果使用 $\epsilon$-貪婪策略（$\epsilon$-greedy），則學習的 Q 值也會考慮這種策略下的行動。

#### **特點**
✅ **適合策略改進（policy improvement）**，因為它直接學習當前策略的行為。
✅ **收斂性較穩定**，因為學到的價值估計與執行行為相匹配。
❌ **探索能力有限**，因為只能學習自己當前策略的數據，難以學習更好的行動。

---

### **2. Off-Policy（外部策略學習）**

#### **定義**
- **學習時的策略與執行時的策略不同**，即可以用**不同的策略來收集數據**，然後用這些數據來學習更好的策略。
- 這允許模型透過**試探性策略（exploration policy）** 來收集數據，但學習一個更優的**目標策略（target policy）**。

#### **代表性演算法**
- **Q-Learning**
  - 無論探索時是否選擇了最佳動作，更新 Q 值時都**假設每個狀態都會選擇最優動作（max Q）**，這使得它可以學習最優策略。

- **Deep Q-Networks（DQN）**
  - 使用 **經驗回放（experience replay）**，存儲過去的數據並從中抽樣來訓練 Q 網絡，使得學習與數據收集分離，這本質上是一種 Off-Policy 方法。

#### **特點**
✅ **探索能力更強**，因為可以使用不同的策略來收集更多多樣的數據。
✅ **可以利用過去經驗數據**，例如 DQN 的**經驗回放（experience replay）**。
❌ **收斂可能不穩定**，因為學習的策略與數據來源可能不匹配。

---

### **3. 直觀舉例**

#### **例子 1：自駕車**
假設你在訓練**自動駕駛車輛**，你的目標是讓車學習到最安全的駕駛方式：
- **On-Policy（SARSA）**：
  - 車輛根據當前駕駛策略行駛，然後學習基於這種駕駛風格的價值函數。例如，如果車輛**偶爾**違規變道，學習到的策略仍然會保留這種行為。
  - 缺點是如果當前策略不夠優秀，學到的內容可能也不夠優秀。

- **Off-Policy（Q-Learning）**：
  - 你讓不同車輛**嘗試各種駕駛風格**（包括安全駕駛和激進駕駛），然後學習**最安全的駕駛策略**。
  - 這樣，即使有些車輛的行為不理想，演算法仍能學習到更好的策略。

---

#### **例子 2：小朋友學騎腳踏車**
- **On-Policy（SARSA）**：
  - 小朋友根據自己當前的學習方式練習，例如**只嘗試自己能做的動作**，然後根據這些嘗試來調整策略。
  - 如果他害怕摔倒，不會嘗試太多冒險的動作，因此學習速度可能較慢。

- **Off-Policy（Q-Learning）**：
  - 小朋友可以**看別人騎腳踏車**，學習到最好的技巧，然後用來改善自己的策略。
  - 這樣，即使自己還沒嘗試某些行動，也可以學習到這些行為的價值。

---

### **4. 總結：On-Policy vs. Off-Policy**
| **比較項目** | **On-Policy（內部策略）** | **Off-Policy（外部策略）** |
|--------------|----------------|----------------|
| **學習策略** | 只能學習自身策略的數據 | 可以從不同策略的數據中學習 |
| **探索能力** | 受限於當前策略的行為 | 可以透過不同策略收集更多數據 |
| **代表演算法** | SARSA, REINFORCE | Q-Learning, DQN |
| **應用場景** | 策略梯度（Policy Gradient）、安全性較高的應用 | 需要長期探索、大量數據的應用 |
| **穩定性** | 通常比較穩定 | 可能會收斂較慢或不穩定 |

**結論：**
- **On-Policy（如 SARSA）適合需要安全性與穩定性的學習場景**，但可能學不到最優策略。
- **Off-Policy（如 Q-Learning）可以利用更多數據來學習更好的策略**，但可能會不穩定或過度偏向最大 Q 值的選擇。

如果你想讓模型學到**最好的策略（最優解）**，通常會選擇 **Off-Policy（如 Q-Learning）**。如果你想要學習穩定且不會意外做出危險決策的策略，則會選擇 **On-Policy（如 SARSA）**。

## What is SARSA & Q-Learning
:::info
詳細數學推導可以參考: [深度強化學習 Ch3.1 : TD learning](https://hackmd.io/@RL666/SkNJdfhn9)
:::
**SARSA（State-Action-Reward-State-Action）** 和 **Q-Learning** 都是強化學習（Reinforcement Learning, RL）中的**值基方法（Value-Based Methods）**，它們的主要目標是**學習動作-價值函數（Q-Function）**，以選擇最佳動作來最大化累積回報。

它們的核心區別在於**策略選擇方式**：
- **SARSA 是 On-Policy（內部策略學習）**，學習當前策略的行為。
- **Q-Learning 是 Off-Policy（外部策略學習）**，學習最優策略，即使數據來自不同的行為策略。

---

### **1. SARSA（On-Policy）**

#### **核心概念**
SARSA 是**On-Policy** 強化學習方法，意味著**它學習的是當前正在執行的策略（policy）**，即它的 Q 值更新是基於實際選擇的動作，而不是理論上的最優動作。

#### **更新公式**
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right]
$$
其中：
- $s_t, a_t, r_t$：當前狀態、動作和獎勵。
- $s_{t+1}, a_{t+1}$：下一個狀態及根據當前策略選擇的下一個動作。
- $\alpha$（學習率）：控制更新步伐。
- $\gamma$（折扣因子）：決定未來回報的重要性。

#### **SARSA 的特點**
✅ **學習當前策略的行為**，不會過於激進地選擇最優解，較穩定。
✅ **策略與學習方式一致**，不會突然變成極端的貪婪策略，適合安全性要求高的環境。
❌ **可能學不到最優策略**，如果策略不是最優的，那學習結果也可能不是最優的。

#### **舉例**
**🛵 例子：學騎腳踏車**
- 小朋友用 SARSA 來學習騎腳踏車，他會根據自己的學習風格來選擇動作（如小心慢騎）。
- 當他遇到坑洞時，他會根據自己當前的策略決定是慢慢避開還是稍微加速躲開。
- 這代表他學習的是一種「安全但可能不是最快的騎車方式」。

---

### **2. Q-Learning（Off-Policy）**

#### **核心概念**
Q-Learning 是**Off-Policy** 強化學習方法，這意味著它**學習的是最優策略，而不管數據來自哪種策略**。
- 即使當前的行為策略（例如隨機選擇動作）不是最優的，Q-Learning 依然會學習最優的行動選擇方式。

#### **更新公式**
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_t + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \right]
$$
與 SARSA 唯一的區別是：
- SARSA 用的是「**實際選擇的下一個動作**」$Q(s_{t+1}, a_{t+1})$。
- Q-Learning 直接選擇「**下一個狀態的最優動作**」$\max_{a} Q(s_{t+1}, a)$ 來更新 Q 值。

#### **Q-Learning 的特點**
✅ **能學到最優策略**，因為它總是選擇最好的 Q 值來學習。
✅ **可以利用 Off-Policy 數據**，例如過去的經驗來更新學習結果（像 DQN 的經驗回放）。
❌ **可能學習不穩定**，如果環境變化大，它可能會過度偏向最優動作而忽略探索。

#### **舉例**
**🚗 例子：自駕車找停車位**
- 假設 Q-Learning 訓練一輛**自動駕駛車**來找停車位。
- 雖然車輛一開始可能亂選路線，但最終它會學到「**最短最快的停車路線**」。
- 即使一開始的數據來自於人類駕駛或隨機行為，Q-Learning 仍然會學習出**最優的停車策略**。

---

### **3. SARSA vs. Q-Learning（比較）**
| **比較項目** | **SARSA（On-Policy）** | **Q-Learning（Off-Policy）** |
|--------------|----------------|----------------|
| **學習策略** | 學習當前執行的策略 | 學習最優策略 |
| **更新方式** | 依據實際選擇的行動更新 | 依據最大 Q 值更新 |
| **穩定性** | 收斂較穩定，但不一定最優 | 可能不穩定，但學習結果更好 |
| **探索能力** | 受限於當前策略的行為 | 可以從不同策略的數據中學習 |
| **代表性應用** | 需要安全性高的應用，如醫療機器人 | 需要找到最優解的應用，如自駕車 |
| **適用場景** | 策略梯度、保守決策環境 | 需要大量探索、目標最優化的環境 |

#### **核心區別**
1. **SARSA 是 On-Policy，學習的是「當前策略」，適合安全性較高的情境（如自駕車減速避讓行人）。**
2. **Q-Learning 是 Off-Policy，學習的是「最優策略」，適合追求最優解的環境（如最快找到停車位）。**

---

### **4. 何時用 SARSA？何時用 Q-Learning？**
✅ **SARSA（On-Policy）適用情境**
- 需要安全性較高的場景，例如：
  - **醫療機器人**（避免風險操作）
  - **自駕車避障**（減少風險行為）
  - **教育 AI**（確保不過於極端行為）
- 主要用於「**學習當前策略的行為**」。

✅ **Q-Learning（Off-Policy）適用情境**
- 需要找到全局最優解，例如：
  - **遊戲 AI**（學到最佳策略）
  - **自駕車尋找最佳路徑**（找最短停車路線）
  - **金融交易策略**（學到最賺錢的投資方式）
- 主要用於「**學習最優策略，而不管數據來源**」。

---

### **5. 總結**
- **SARSA：On-Policy，學當前策略，較安全但可能不是最優解。**
- **Q-Learning：Off-Policy，學最優策略，探索能力強但可能學習不穩定。**
- **如果你希望 AI 探索最優解，選 Q-Learning；如果你希望 AI 行為穩定，選 SARSA。**