<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NTU_ML :: Tag :: SBK Hugo Site</title><link>https://bernie6401.github.io/tags/ntu_ml/index.html</link><description/><generator>Hugo</generator><language>en-us</language><atom:link href="https://bernie6401.github.io/tags/ntu_ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Machine Learning Hand-write Homework &amp; Answer</title><link>https://bernie6401.github.io/security/course/ntu-ml/machine-learning-hand-write-homework--answer/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://bernie6401.github.io/security/course/ntu-ml/machine-learning-hand-write-homework--answer/index.html</guid><description>Machine Learning Hand-write Homework &amp; Answer tags: NTU_ML Machine Learning HW1 HW1-Ans HW2 HW2-Ans HW3 HW3-Ans HW4 HW4-Ans</description></item><item><title>NTU Machine Learning Final Project Proposal Notes</title><link>https://bernie6401.github.io/security/course/ntu-ml/ntu-machine-learning-final-project-proposal-notes-1/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://bernie6401.github.io/security/course/ntu-ml/ntu-machine-learning-final-project-proposal-notes-1/index.html</guid><description>NTU Machine Learning Final Project Proposal Notes tags: NTU_ML Machine Learning Paper Used Technique / Ingenuity Suitable / Unsuitable Reason Replace to [1] Models overview3D maps of gray and/or white matter (deep learning models: six layer CNN, ResNet, and Inception V1)vertex wise measurements from the surface-based processing (models BLUP and SVM)Model 1: Best Linear Unbiased Predictor(BLUP)Model 2: Support Vector RegressionModel 3: Six-Layer Convolutional Neural NetworksModel 4: Specialized Six-Layer Convolutional Neural Networks for Younger and Older SubjectsModel 5: ResNetModel 6: Inception V1 Additional ExperimentsDifferent Types of Model Combination: Linear Regression vs. Random ForestCombining Seven (Identical) Convolutional Neural Networks or the Seven Best EpochsInfluence of the Type of Brain Features on Prediction Accuracy Suitable:In this field, it’s very clearly on comparing 6 variety models which can help us to know the implementation what we learned in class.Also can aware of the result between high level model and custom level modelFor linear regression and random forest, they trained the ensemble algorithms on a random subset. They repeated this process 500 times to get a bootstrap estimate of the SE of the MAE. N/A [2] 2D and 3D-CNN on age estimationFor 2D-CNN, we consider the features as an image of size 168×60 (DH×M) ignoring the days as temporal information.However, for 3D-CNN, we consider the features as a 3D volume with temporal information across the days, where each day has 24 hours and an hour is 60 minutes. So to break it down, we represent the features as a three dimensional information of 7×24×60 (D×H×M) minutes. Unsuitable: Though the topic is interesting, the technique content is less then expectation and the . No Idea Yet [3] Model for classification:Random Forest, GLMNet, SVM(including e1071, which is a package of LibSVM in R language, LiblinearR, kernlab, Rgtsvm), and xgboostCalibration Algorithm(i.e. post-processing):logistic regression(GLM function), BRGLM, GLMNetPerformance evaluation: HandTill2001 Suitable:The reason is as the same as [1] which also used various methods and compare it to other papers detailed. N/A Appendix The custom model in [1]</description></item><item><title>NTU Machine Learning Final Project Proposal Notes</title><link>https://bernie6401.github.io/security/course/ntu-ml/ntu-machine-learning-final-project-proposal-notes/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://bernie6401.github.io/security/course/ntu-ml/ntu-machine-learning-final-project-proposal-notes/index.html</guid><description>NTU Machine Learning Final Project Proposal Notes tags: NTU_ML Machine Learning Deep6mAPred: A CNN and Bi-LSTM-based deep learning method for predicting DNA N6-methyladenosine sites across plant species Introduction &amp; Motivation DNA methylation alters activities of DNA segments without changing the sequence, which thus yields a wide variety of roles in the cellular processes across organisms or tissues DNA methylation is widely distributed both in prokaryote and in eukaryote, but the proportion of methylated residues differs greatly with species DNA methylation is essential for normal development DNA methylation is increasingly attracting attentions from biologists Related works Even Luo et al.[41] - proposed the DNA 6mA as a new epigenetic mark in eukaryotes</description></item><item><title>NTU Machine Learning Homework 1</title><link>https://bernie6401.github.io/security/course/ntu-ml/ntu-machine-learning-homework-1/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://bernie6401.github.io/security/course/ntu-ml/ntu-machine-learning-homework-1/index.html</guid><description>NTU Machine Learning Homework 1 tags: NTU_ML Machine Learning How to choose features of data After observing the training data visualized image, you can be aware of the relationship between the PM2.5 feature and the others. For instance, the CO image, NO image, NO2 image, and NOx image are much more correlated with PM2.5. I also choose PM10, WS_HR, RAINFALL, RH, WIND_SPEED, and PM2.5 which you can see here I used Zscore normalization to implement in my project and can see as below You can see the different result of using or unusing normalization with the same config. Epoch Regression LR Feats Batch Size Loss Fn. Opti. RMSE Data Filter Norm. Data 200 1st-order 0.015 [1-4, 6-9, 13, 14] 1024 MSE Adam 2.44623 Yes Yes 200 1st-order 0.015 [1-4, 6-9, 13, 14] 1024 MSE Adam 2.44623 Yes No Hyperparameter and Preprocessing All my testing config can be found in Training Result.xlsx I used a filter to choose valid data and set a threshold by observing the visualized figure of all features. My takeaway **(Solved->See the last paragraph)**Using normalization is not like what I thought. Practically speaking, using normalization can gather all data to a specific area that the model can converge much more rapidly. But, in this case, the result is worse and also appear negative value of the PM2.5 result. According to this page, maybe the normalization method is not suitable in my case. **(Solved->See the last paragraph)**I also figured that using the stored weight and bias by my pretrained model is not the right way. I used pickle to store the dump parameters during the training and used the best one as my pretrained parameter. But it’s still not that good enough. The better way in this project to enhance your accuracy is tuning your training config and select good features. After discussing with my friend, I figured out the problem and tried to solve it successfully by fitting numpy random seed. Then, the parameter will truly fix but normalization is still not working to help model converging. Update 2022/12/06 update - Refer to 相關 taught by Dr.李柏堅, I use Pearson Correlation to compute the correlation of each factor and PM2.5 and the result is shown as below. According to the video, |r| &lt; 0.4 is low correlation, 0.4 ≦ |r| &lt; 0.7is medium correlation, and 0.7 ≦ |r| &lt; 1 is high correlation. So, the factor CO, NO, NO2, NOx, PM10, and SO2 are quite suitable as our input data to address this regression problem. Factor AMB_TEMP CO NO NO2 NOx O3 PM10 WS_HR RAINFALL RH SO2 WD_HR WIND_DIREC WIND_SPEED r -0.176147465 0.659147668 0.227219147 0.554273687 0.51365014 0.233923944 0.818868214 -0.102047405 -0.060801221 -0.081576429 0.361333416 0.171932397 0.137658351 -0.10119696</description></item><item><title>NTU Machine Learning Homework 2</title><link>https://bernie6401.github.io/security/course/ntu-ml/ntu-machine-learning-homework-2/index.html</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://bernie6401.github.io/security/course/ntu-ml/ntu-machine-learning-homework-2/index.html</guid><description>NTU Machine Learning Homework 2 tags: NTU_ML Machine Learning :::spoiler Click to open TOC [TOC] :::
Objective We’d like to classify human-being emotion by using CNN model that self-construct or others ready-made such as ResNet or VGG model.
Data We used emotional dataset from FER2013 that were preprocessed by lecture TA.
Models Originial
self.conv_0 = nn.Sequential( nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64, eps=1e-05, affine=True), nn.LeakyReLU(negative_slope=0.05), nn.MaxPool2d((2, 2)), ) I’ve used 3-level model for training but not have good result</description></item></channel></rss>