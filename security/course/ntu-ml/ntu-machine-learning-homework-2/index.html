<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  NTU Machine Learning Homework 2
  #


  tags: NTU_ML Machine Learning
  #

:::spoiler Click to open TOC
[TOC]
:::

  Objective
  #

We&rsquo;d like to classify human-being emotion by using CNN model that self-construct or others ready-made such as ResNet or VGG model.

  Data
  #

We used emotional dataset from FER2013 that were preprocessed by lecture TA.

  Models
  #



Originial
self.conv_0 = nn.Sequential(
    nn.Conv2d(1, 64, kernel_size=3, padding=1),
    nn.BatchNorm2d(64, eps=1e-05, affine=True),
    nn.LeakyReLU(negative_slope=0.05),
    nn.MaxPool2d((2, 2)),
)


I&rsquo;ve used 3-level model for training but not have good result"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://bernie6401.github.io/security/course/ntu-ml/ntu-machine-learning-homework-2/"><meta property="og:site_name" content="SBK Hugo Site"><meta property="og:title" content="NTU Machine Learning Homework 2"><meta property="og:description" content="NTU Machine Learning Homework 2 # tags: NTU_ML Machine Learning # :::spoiler Click to open TOC [TOC] :::
Objective # We’d like to classify human-being emotion by using CNN model that self-construct or others ready-made such as ResNet or VGG model.
Data # We used emotional dataset from FER2013 that were preprocessed by lecture TA.
Models # Originial
self.conv_0 = nn.Sequential( nn.Conv2d(1, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64, eps=1e-05, affine=True), nn.LeakyReLU(negative_slope=0.05), nn.MaxPool2d((2, 2)), ) I’ve used 3-level model for training but not have good result"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="security"><meta property="article:tag" content="NTU_ML"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="NTU"><title>NTU Machine Learning Homework 2 | SBK Hugo Site</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://bernie6401.github.io/security/course/ntu-ml/ntu-machine-learning-homework-2/><link rel=stylesheet href=/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.acdc41c8d39e6c69d70d8a23779875e0a3733fefead3e428d5344966bb12f562.js integrity="sha256-rNxByNOebGnXDYojd5h14KNzP+/q0+Qo1TRJZrsS9WI=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>SBK Hugo Site</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>NTU Machine Learning Homework 2</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li></li></ul></li><li><a href=#objective>Objective</a></li><li><a href=#data>Data</a></li><li><a href=#models>Models</a></li><li><a href=#other-technique-i-used>Other Technique I used</a></li><li><a href=#environment>Environment</a></li><li><a href=#run>Run</a></li><li><a href=#result>Result</a><ul><li><a href=#early-stopping>Early-Stopping</a></li><li><a href=#data-augmentation>Data Augmentation</a></li><li><a href=#confusion-matrix>Confusion Matrix</a></li><li><a href=#data-distribution>Data Distribution</a></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=ntu-machine-learning-homework-2>NTU Machine Learning Homework 2
<a class=anchor href=#ntu-machine-learning-homework-2>#</a></h1><h6 id=tags-ntu_ml-machine-learning>tags: <code>NTU_ML</code> <code>Machine Learning</code>
<a class=anchor href=#tags-ntu_ml-machine-learning>#</a></h6><p>:::spoiler Click to open TOC
[TOC]
:::</p><h2 id=objective>Objective
<a class=anchor href=#objective>#</a></h2><p>We&rsquo;d like to classify human-being emotion by using CNN model that self-construct or others ready-made such as ResNet or VGG model.</p><h2 id=data>Data
<a class=anchor href=#data>#</a></h2><p>We used emotional dataset from <a href="https://www.kaggle.com/datasets/msambare/fer2013?datasetId=786787&amp;sortBy=dateRun&amp;tab=profile">FER2013</a> that were preprocessed by lecture TA.</p><h2 id=models>Models
<a class=anchor href=#models>#</a></h2><ul><li><p>Originial</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>self<span style=color:#f92672>.</span>conv_0 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>64</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>BatchNorm2d(<span style=color:#ae81ff>64</span>, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-05</span>, affine<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>LeakyReLU(negative_slope<span style=color:#f92672>=</span><span style=color:#ae81ff>0.05</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>MaxPool2d((<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)),
</span></span><span style=display:flex><span>)
</span></span></code></pre></div></li><li><p>I&rsquo;ve used 3-level model for training but not have good result</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>self<span style=color:#f92672>.</span>conv_3layer <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>1</span>, n_chansl, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>BatchNorm2d(n_chansl, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-05</span>, affine<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>LeakyReLU(negative_slope<span style=color:#f92672>=</span><span style=color:#ae81ff>0.05</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>MaxPool2d((<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)),   <span style=color:#75715e># (Batch_size, 32, 32, 32)-&gt;(B, C, H, W)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Conv2d(n_chansl, n_chansl<span style=color:#f92672>//</span><span style=color:#ae81ff>2</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>BatchNorm2d(n_chansl<span style=color:#f92672>//</span><span style=color:#ae81ff>2</span>, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-05</span>, affine<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>LeakyReLU(negative_slope<span style=color:#f92672>=</span><span style=color:#ae81ff>0.05</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>MaxPool2d((<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)),   <span style=color:#75715e># (Batch_size, 64, 16, 16)-&gt;(B, C, H, W)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Conv2d(n_chansl<span style=color:#f92672>//</span><span style=color:#ae81ff>2</span>, n_chansl<span style=color:#f92672>//</span><span style=color:#ae81ff>4</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>BatchNorm2d(n_chansl<span style=color:#f92672>//</span><span style=color:#ae81ff>4</span>, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-05</span>, affine<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>LeakyReLU(negative_slope<span style=color:#f92672>=</span><span style=color:#ae81ff>0.05</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>MaxPool2d((<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)),   <span style=color:#75715e># (Batch_size, 128, 8, 8)-&gt;(B, C, H, W)</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>fc_3layer <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Linear(n_chansl<span style=color:#f92672>//</span><span style=color:#ae81ff>4</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>7</span>),
</span></span><span style=display:flex><span>)
</span></span></code></pre></div></li><li><p>I&rsquo;ve also used 4-layer that the channel increase in the first three layers and decrease the channel at the last layer but still not good enough</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>self<span style=color:#f92672>.</span>conv_4layer <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Conv2d(<span style=color:#ae81ff>1</span>, n_chansl, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>BatchNorm2d(n_chansl, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-05</span>, affine<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>LeakyReLU(negative_slope<span style=color:#f92672>=</span><span style=color:#ae81ff>0.05</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>MaxPool2d((<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)),    <span style=color:#75715e># (Batch_size, n_chansl, 32, 32)-&gt;(B, C, H, W)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Conv2d(n_chansl, n_chansl<span style=color:#f92672>*</span><span style=color:#ae81ff>2</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>BatchNorm2d(n_chansl<span style=color:#f92672>*</span><span style=color:#ae81ff>2</span>, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-05</span>, affine<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>LeakyReLU(negative_slope<span style=color:#f92672>=</span><span style=color:#ae81ff>0.05</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>MaxPool2d((<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)),    <span style=color:#75715e># (Batch_size, n_chansl*2, 16, 16)-&gt;(B, C, H, W)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Conv2d(n_chansl<span style=color:#f92672>*</span><span style=color:#ae81ff>2</span>, n_chansl<span style=color:#f92672>*</span><span style=color:#ae81ff>4</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>BatchNorm2d(n_chansl<span style=color:#f92672>*</span><span style=color:#ae81ff>4</span>, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-05</span>, affine<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>LeakyReLU(negative_slope<span style=color:#f92672>=</span><span style=color:#ae81ff>0.05</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>MaxPool2d((<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)),    <span style=color:#75715e># (Batch_size, n_chansl*4, 8, 8)-&gt;(B, C, H, W)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Conv2d(n_chansl<span style=color:#f92672>*</span><span style=color:#ae81ff>4</span>, n_chansl<span style=color:#f92672>*</span><span style=color:#ae81ff>2</span>, kernel_size<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, padding<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>BatchNorm2d(n_chansl<span style=color:#f92672>*</span><span style=color:#ae81ff>2</span>, eps<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-05</span>, affine<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>LeakyReLU(negative_slope<span style=color:#f92672>=</span><span style=color:#ae81ff>0.05</span>),
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>MaxPool2d((<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>)),    <span style=color:#75715e># (Batch_size, n_chansl*2, 4, 4)-&gt;(B, C, H, W)</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>fc_4layer <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>    nn<span style=color:#f92672>.</span>Linear(n_chansl<span style=color:#f92672>*</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>7</span>),
</span></span><span style=display:flex><span>)
</span></span></code></pre></div></li><li><p>4-Level New is similar to previous version but double the channel size and always increasing. Then the result is not bad.</p><pre tabindex=0><code class="language-python=" data-lang="python=">self.conv_4layer = nn.Sequential(
    nn.Conv2d(1, n_chansl, kernel_size=3, padding=1),
    nn.BatchNorm2d(n_chansl, eps=1e-05, affine=True),
    nn.LeakyReLU(negative_slope=0.05),
    nn.MaxPool2d((2, 2)),   # (Batch_size, n_chansl, 32, 32)-&gt;(B, C, H, W)

    nn.Conv2d(n_chansl, n_chansl*4, kernel_size=3, padding=1),
    nn.BatchNorm2d(n_chansl*4, eps=1e-05, affine=True),
    nn.LeakyReLU(negative_slope=0.05),
    nn.MaxPool2d((2, 2)),   # (Batch_size, n_chansl*4, 16, 16)-&gt;(B, C, H, W)

    nn.Conv2d(n_chansl*4, n_chansl*8, kernel_size=3, padding=1),
    nn.BatchNorm2d(n_chansl*8, eps=1e-05, affine=True),
    nn.LeakyReLU(negative_slope=0.05),
    nn.MaxPool2d((2, 2)),   # (Batch_size, n_chansl*8, 8, 8)-&gt;(B, C, H, W)

    nn.Conv2d(n_chansl*8, n_chansl*16, kernel_size=3, padding=1),
    nn.BatchNorm2d(n_chansl*16, eps=1e-05, affine=True),
    nn.LeakyReLU(negative_slope=0.05),
    nn.MaxPool2d((2, 2)),   # (Batch_size, n_chansl*16, 4, 4)-&gt;(B, C, H, W)
)
self.fc_4layer = nn.Sequential(
    nn.Linear(n_chansl*16 * 4 * 4, n_chansl*4 * 4 * 4),
    nn.Linear(n_chansl*4 * 4 * 4, 7)
)
</code></pre></li></ul><h2 id=other-technique-i-used>Other Technique I used
<a class=anchor href=#other-technique-i-used>#</a></h2><ul><li>Early-Stopping</li><li>Normalization: you can find the code that I compute the mean and standard deviation in <code>temp.py</code>.</li><li>Data Augmentation<ul><li>[Ver. 1] including RandomChoice from RandomHorizontalFlip, ColorJitter, RandomRotation and used CenterCrop to a specific size then used Pad to original size. This version is for <strong>self-defined model</strong>.</li></ul></li><li>Plot Confusion Matrix: must command <code>--plot_cm</code> in cmd</li><li>Visualize Data Distribution by bar chart.</li></ul><h2 id=environment>Environment
<a class=anchor href=#environment>#</a></h2><pre tabindex=0><code>conda install -c conda-forge argparse
conda install -c conda-forge tqdm
conda install -c conda-forge wandb
conda install -c anaconda more-itertools
conda install -c anaconda scikit-learn
conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge
</code></pre><h2 id=run>Run
<a class=anchor href=#run>#</a></h2><ul><li>We supply some self-defined arguments such as basic <code>--epochs</code>, <code>--lr</code>, <code>--batch_size</code>, <code>--val_batch_size</code>, <code>--checkpoint</code>.</li><li>And we also supply advanced setting like <code>--optimizer</code> including Adam and SGD, <code>--weight_d</code>, <code>--momentum</code>, <code>--gamma</code> and <code>--step</code> for learning rate scheduler, <code>--channel_num</code> for model channel numbers.</li><li>Other tools such as <code>--wandb</code> which is a visualized and logging tool to record every things you want to update on <a href=https://wandb.ai/>website</a>, and <code>--plot_cm</code> to visualize validation result.</li><li>For training with using data augmentation, scheduler and early stopping<pre tabindex=0><code>python MLHW.py  --epoch 600 --lr 0.001 --gamma 0.2 --step 40 --batch_size 256 --early_stop --data_aug -c ./epoch490_acc0.6243.pth
</code></pre></li><li>For testing<pre tabindex=0><code>python MLHW.py --mode test -c ./epoch115_acc0.6318.pth
</code></pre></li></ul><h2 id=result>Result
<a class=anchor href=#result>#</a></h2><ul><li>Whole result with the configuration and technique above is <a href="https://wandb.ai/bernie6401/MLHW2/overview?workspace=user-bernie6401">here</a>.</li></ul><h3 id=early-stopping>Early-Stopping
<a class=anchor href=#early-stopping>#</a></h3><p>As you can see below, if I use early stopping technique, it&rsquo;ll break the training loop when overfitting. The orange line is what I set early stopping with threshold 5. That is, if the model loss rise up 5 times consequently, then stop training. The other one doesn&rsquo;t set early stopping and you can see it&rsquo;ll complete the training loop even overfitting occur. <img src=https://imgur.com/FrJElIp.png alt=early_stop_train_acc><img src=https://imgur.com/GlFBuhu.png alt=early_stop_train_loss><img src=https://imgur.com/xmTOt04.png alt=early_stop_val_acc><img src=https://imgur.com/9M8Wdwa.png alt=early_stop_val_loss></p><h3 id=data-augmentation>Data Augmentation
<a class=anchor href=#data-augmentation>#</a></h3><p>As you can see below, if I use data augmentation, it can conquer overfitting. The other configurations are the same and the breakpoint of orange line is because of early-stopping. I set data augmentation technique on purple one and the others didn&rsquo;t.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>transform_set <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    transforms<span style=color:#f92672>.</span>RandomHorizontalFlip(p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>),   <span style=color:#75715e># Horizontal Flip in random</span>
</span></span><span style=display:flex><span>    transforms<span style=color:#f92672>.</span>ColorJitter(brightness<span style=color:#f92672>=</span>(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>5</span>), contrast<span style=color:#f92672>=</span>(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>5</span>), saturation<span style=color:#f92672>=</span>(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>5</span>), hue<span style=color:#f92672>=</span>(<span style=color:#f92672>-</span><span style=color:#ae81ff>0.1</span>, <span style=color:#ae81ff>0.1</span>)),  <span style=color:#75715e># Adjust image brightness, contrast, satuation and hue in random</span>
</span></span><span style=display:flex><span>    transforms<span style=color:#f92672>.</span>RandomRotation(<span style=color:#ae81ff>30</span>, center<span style=color:#f92672>=</span>(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>), expand<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>),]   <span style=color:#75715e># expand only for center rotation</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>transform_aug <span style=color:#f92672>=</span> transforms<span style=color:#f92672>.</span>Compose([
</span></span><span style=display:flex><span>    transforms<span style=color:#f92672>.</span>RandomChoice(transform_set),
</span></span><span style=display:flex><span>    transforms<span style=color:#f92672>.</span>Resize(<span style=color:#ae81ff>224</span>)])
</span></span></code></pre></div><p>I choose RandomChoice to choose transform_set including RandomHorizontalFlip, ColorJitter, RandomRotation. ColorJitter will adjust the brightness, contrast, saturation and hue of the input image randomly. So, it can increase the diversity of training dataset properly. Though, lecture TA is not very suggestive to use RandomVerticalFlip skill on training image, because it&rsquo;ll transform the image that no human can recognize it. So, I use RandomHorizontalFlip instead.
<img src=https://imgur.com/PL2Ykmq.png alt=data_aug_train_acc><img src=https://imgur.com/V6NxzMD.png alt=data_aug_train_loss><img src=https://imgur.com/aOtnNaf.png alt=data_aug_val_acc><img src=https://imgur.com/sewt3kI.png alt=data_aug_val_loss></p><h3 id=confusion-matrix>Confusion Matrix
<a class=anchor href=#confusion-matrix>#</a></h3><p>As you can see the confusion matrix below. The second class(emotion Disgust) is the worst result of the classification and the Happy class is the best. Also, the Fear class is not good enough. I think the main reason is data imbalance that shown below of second one. The prior of these two classes are 0.0155 and 0.1443 respectively. Under this circumstance, the model can&rsquo;t learn this class by enough images properly. And the bad result of Fear class. I think it&rsquo;s just not learn very well with bad model structure and bad configuration.
<img src=https://imgur.com/JXcbEKx.png alt=confusion_matrix>
<img src=https://imgur.com/SEHelPa.png alt=data_distribution></p><h3 id=data-distribution>Data Distribution
<a class=anchor href=#data-distribution>#</a></h3><p>The result of data distribution is shown above. The prior probability of the highest probability is $6525/25887=0.252$. If not targeting a specific category and just choose the Happy class, it would be worse than normal classification progress.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li></li></ul></li><li><a href=#objective>Objective</a></li><li><a href=#data>Data</a></li><li><a href=#models>Models</a></li><li><a href=#other-technique-i-used>Other Technique I used</a></li><li><a href=#environment>Environment</a></li><li><a href=#run>Run</a></li><li><a href=#result>Result</a><ul><li><a href=#early-stopping>Early-Stopping</a></li><li><a href=#data-augmentation>Data Augmentation</a></li><li><a href=#confusion-matrix>Confusion Matrix</a></li><li><a href=#data-distribution>Data Distribution</a></li></ul></li></ul></nav></div></aside></main></body></html>