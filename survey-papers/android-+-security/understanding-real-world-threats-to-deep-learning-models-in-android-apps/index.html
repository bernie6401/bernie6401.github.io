<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Understanding Real-world Threats to Deep Learning Models in Android Apps
  #


  tags: Meeting Paper NTU
  #

:::info
Deng, Z., Chen, K., Meng, G., Zhang, X., Xu, K., & Cheng, Y. (2022, November). Understanding real-world threats to deep learning models in android apps. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security (pp. 785-799).
:::

  Background
  #

:::spoiler What is Adversarial Example? - 運用對抗例攻擊深度學習模型

所謂對抗例，是一種刻意製造的、讓機器學習模型判斷錯誤的輸入資料。最早是 Szegedy et al（2013）發現對於用 ImageNet、AlexNet 等資料集訓練出來的影像辨識模型，常常只需要輸入端的微小的變動，就可以讓輸出結果有大幅度的改變。例如取一張卡車的照片，可以被模型正確辨識，但只要改變影像中的少數像素，就可以讓模型辨識錯誤，而且前後對影像的改變非常少，對肉眼而言根本分不出差異。
:::"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://bernie6401.github.io/survey-papers/android-+-security/understanding-real-world-threats-to-deep-learning-models-in-android-apps/"><meta property="og:site_name" content="SBK Hugo Site"><meta property="og:title" content="Understanding Real-world Threats to Deep Learning Models in Android Apps"><meta property="og:description" content="Understanding Real-world Threats to Deep Learning Models in Android Apps # tags: Meeting Paper NTU # :::info Deng, Z., Chen, K., Meng, G., Zhang, X., Xu, K., & Cheng, Y. (2022, November). Understanding real-world threats to deep learning models in android apps. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security (pp. 785-799). :::
Background # :::spoiler What is Adversarial Example? - 運用對抗例攻擊深度學習模型
所謂對抗例，是一種刻意製造的、讓機器學習模型判斷錯誤的輸入資料。最早是 Szegedy et al（2013）發現對於用 ImageNet、AlexNet 等資料集訓練出來的影像辨識模型，常常只需要輸入端的微小的變動，就可以讓輸出結果有大幅度的改變。例如取一張卡車的照片，可以被模型正確辨識，但只要改變影像中的少數像素，就可以讓模型辨識錯誤，而且前後對影像的改變非常少，對肉眼而言根本分不出差異。 :::"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="survey-papers"><meta property="article:tag" content="Meeting Paper"><meta property="article:tag" content="NTU"><title>Understanding Real-world Threats to Deep Learning Models in Android Apps | SBK Hugo Site</title>
<link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://bernie6401.github.io/survey-papers/android-+-security/understanding-real-world-threats-to-deep-learning-models-in-android-apps/><link rel=stylesheet href=/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.acdc41c8d39e6c69d70d8a23779875e0a3733fefead3e428d5344966bb12f562.js integrity="sha256-rNxByNOebGnXDYojd5h14KNzP+/q0+Qo1TRJZrsS9WI=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>SBK Hugo Site</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label><h3>Understanding Real-world Threats to Deep Learning Models in Android Apps</h3><label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li></li></ul></li><li><a href=#background>Background</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=understanding-real-world-threats-to-deep-learning-models-in-android-apps>Understanding Real-world Threats to Deep Learning Models in Android Apps
<a class=anchor href=#understanding-real-world-threats-to-deep-learning-models-in-android-apps>#</a></h1><h6 id=tags-meeting-paper-ntu>tags: <code>Meeting Paper</code> <code>NTU</code>
<a class=anchor href=#tags-meeting-paper-ntu>#</a></h6><p>:::info
Deng, Z., Chen, K., Meng, G., Zhang, X., Xu, K., & Cheng, Y. (2022, November). Understanding real-world threats to deep learning models in android apps. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security (pp. 785-799).
:::</p><h2 id=background>Background
<a class=anchor href=#background>#</a></h2><p>:::spoiler <a href=https://medium.com/trustableai/%E9%87%9D%E5%B0%8D%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E7%9A%84%E6%83%A1%E6%84%8F%E8%B3%87%E6%96%99%E6%94%BB%E6%93%8A-%E4%B8%80-e94987742767>What is Adversarial Example? - 運用對抗例攻擊深度學習模型</a></p><blockquote><p>所謂對抗例，是一種刻意製造的、讓機器學習模型判斷錯誤的輸入資料。最早是 Szegedy et al（2013）發現對於用 ImageNet、AlexNet 等資料集訓練出來的影像辨識模型，常常只需要輸入端的微小的變動，就可以讓輸出結果有大幅度的改變。例如取一張卡車的照片，可以被模型正確辨識，但只要改變影像中的少數像素，就可以讓模型辨識錯誤，而且前後對影像的改變非常少，對肉眼而言根本分不出差異。
:::</p></blockquote><p>:::spoiler <a href=https://blog.csdn.net/chehec2010/article/details/91360772>hook（钩子函数）</a>
<a href=https://www.zixuerumen.com/17234.html>钩子函数是什么意思</a></p><blockquote><p>在Windows系統中一切皆消息，按鍵盤上的鍵，也是一個消息。Hook 的意思是鉤住，也就是在消息過去之前，先把消息鉤住，不讓其傳遞，使用戶可以優先處理。執行這種操作的函數也稱為鉤子函數。</p></blockquote><p><a href=https://www.fineart-tech.com/index.php/ch/news/699-fineartsecurity-apihook>Hook API讓應用程式乖乖轉彎，駭客也是這麼做 </a>:::</p><p>:::spoiler <a href=https://www.geeksforgeeks.org/remote-procedure-call-rpc-in-operating-system/>Remote Procedure Call (RPC) in Operating System</a></p><blockquote><p>Remote Procedure Call (RPC) is a powerful technique for constructing distributed, client-server based applications. It is based on extending the conventional local procedure calling so that the called procedure need not exist in the same address space as the calling procedure. The two processes may be on the same system, or they may be on different systems with a network connecting them.</p></blockquote><hr><p>在王凡老師的OS中也有提到RPC(Ch.3 P3.54)</p><blockquote><p>Remote procedure call abstract procedure calls between processes on networked systems</p></blockquote><p>簡單來說，他可以執行遠端PC的某一個module或method，而這東西的好處是可以降低programmer學習IPC的障礙，因為這種方式更直觀，大概就像下圖一樣
<img src=https://media.geeksforgeeks.org/wp-content/uploads/operating-system-remote-procedure-call-1.png alt>
:::</p><p>:::spoiler <a href=https://www.researchgate.net/figure/Example-Class-Hierarchy-Analysis-CHA-Our-Class-Hierarchy-Analysis-is-a-static-compile_fig1_269196977>What is Class Hierarchy Analysis?</a></p><blockquote><p>It is a static (compile time) analysis that uses the class hierarchy to compute which method implementations can be invoked by objects of each class type. The left diagram above shows an example hierarchy of five classes where subclasses point to their parent class: D and E are subclasses of C while B and C are subclasses of A.
<img src=https://www.researchgate.net/profile/Zachary-Tatlock/publication/269196977/figure/fig1/AS:668907362856968@1536491351260/Example-Class-Hierarchy-Analysis-CHA-Our-Class-Hierarchy-Analysis-is-a-static-compile.png alt>
:::</p></blockquote><p><a href=https://blog.csdn.net/zw0Pi8G5C1x/article/details/121571055>8 種主流深度學習框架介紹</a>
<a href=https://ithelp.ithome.com.tw/articles/10272501>[Day 21] 媽! Keras 和 TensorFlow 在亂存模型啦! ( TFLite 輕量模型)</a></p><p>:::spoiler <a href=https://d246810g2000.medium.com/%E6%96%87%E5%AD%97%E8%BE%A8%E8%AD%98%E6%96%B9%E6%B3%95%E7%B5%B1%E6%95%B4-1e3d3ba5fe54>What is OCR? - 文字辨識方法統整</a></p><blockquote><p>OCR 英文全稱是 Optical Character Recognition，中文叫做光學字元識別，目前是文字辨識的統稱，已不限於文檔或書本文字辨識，更包括辨識自然場景下的文字，又可以稱為 STR（Scene Text Recognition）。</p><p>圖1 中有三個大分類，包含 Text detection, Text recognition, Text spotting，Text detection 主要是偵測文字在影像中的哪個位置，Text recognition 主要是將偵測後的結果拿來辨識是什麼文字，而 Text spotting 則是將 detection 和 recognition 整合到一個 End-to-End 的網路中來進行文字辨識。
<img src=https://miro.medium.com/v2/resize:fit:720/format:webp/1*UxmtG_Y3E4NyZVeoGnD3OQ.png alt>
:::</p></blockquote><p><a href=https://ithelp.ithome.com.tw/articles/10233788>互聯網行業中，常說的API和SDK是什麼？</a></p><p><a href=https://www.jianshu.com/p/4272e0805da3>What is Tiny Encryption Algorithm(TEA)?</a></p><p><a href=https://blog.csdn.net/tugouxp/article/details/123262864>What is MACE framework? - 小米AI推理框架MACE介绍</a></p><p>:::spoiler <a href=https://www.techtarget.com/searchitoperations/definition/trusted-execution-environment-TEE>What is a trusted execution environment (TEE)?</a></p><blockquote><p>A trusted execution environment (TEE) is an area on the main processor of a device that is separated from the system&rsquo;s main operating system (OS). It ensures data is stored, processed and protected in a secure environment. TEEs provide protection for anything connected, such as a trusted application (TA), by enabling an isolated, cryptographic electronic structure and end-to-end security. This includes the execution of authenticated code, confidentiality, authenticity, privacy, system integrity and data access rights.
:::</p></blockquote><p>:::spoiler What is Perturbation Budget? - From ChatGPT</p><blockquote><p>在深度學習安全領域，擾動預算指的是可以引入到輸入數據中的最大擾動或失真程度，而不會顯著影響深度學習模型的輸出或預測結果。</p><p>擾動通常作為對抗攻擊的一部分引入，攻擊者試圖以某種方式操縱輸入數據，使模型出現誤分類或產生錯誤輸出。通過設置擾動預算，系統可以限制這些攻擊的影響，並提高其對抗攻擊的魯棒性。</p><p>擾動預算的定義方式因應用和攻擊類型而異。例如，它可以用擾動向量的L2或L∞範數來衡量，分別代表原始輸入數據和擾動後數據之間的歐幾里得距離或最大絕對差值。</p><p>總的來說，擾動預算是評估深度學習模型安全性和魯棒性的重要參數，特別是在安全性是重要關注點的應用中。
:::</p></blockquote><p>:::spoiler <a href=https://datasciocean.tech/machine-learning-basic-concept/machine-learning-model-inference/>What is quantization? - 使用機器學習解決問題的五步驟 : 模型推論</a></p><blockquote><p>Pruning 與 Quantization</p><p>我們在這裡簡單說明 Pruning 與 Quantization 的概念，如果想更深入學習模型效能、速度與能耗的最佳化問題，可以參考 TensorFlow 的官方文件。</p><p>Pruning : 全名為 Weight Pruning，中文稱為「權重修剪」。透過觀察模型中哪些參數對於模型的預測過程較沒有影響，將這些參數移除，達到降低模型複雜度與運算量的目的。
Quantization : 中文稱為「量化」。模型中的參數如果是 32-bit 的浮點數，將其轉為 8-bit。透過簡化模型中參數的「精確程度」達到降低模型體積並提高運算速度的目的。
不管是 Pruning 或是 Quantization，都是希望夠在簡化模型複雜度、提升運算速度並降低能源與時間消耗的同時，保持模型原來的預測準確度。
:::</p></blockquote><p><a href=https://youtu.be/qD6iD4TFsdQ>What is transfer learning?</a></p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li></li></ul></li><li><a href=#background>Background</a></li></ul></nav></div></aside></main></body></html>